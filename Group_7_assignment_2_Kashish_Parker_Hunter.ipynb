{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "Objective:\n",
    "To determine the variety of date fruit from data describing the colour, length, diameter, and shape.\n",
    "\n",
    "Data:\n",
    "Obtained from DATASETS (muratkoklu.com) and used in M. Koklu, R. Kursun, Y.S. Taspinar, and I. Cinar, \"Classification of Date Fruits into Genetic Varieties Using Image Analysis,\" Mathematical Problems in Engineering, Vol.2021, Article ID: 4793293 (2021).\n",
    "\n",
    "Problem Statement:\n",
    "In food production it is important to properly label ingredients for both health and business reasons. However, sometimes mistakes are made and there is room for improvement in food labeling practices. A number of different types of dates are grown around the world, and it takes expertise to correctly identify the variety. Your job as a machine learning developer is to create a model that can identify the type of date from external features such as colour, length, diameter and shape factors which have been determined by a computer vision model.\n",
    "\n",
    "Steps to be completed:\n",
    "Create a Jupyter notebook and complete the following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data\n",
    "Load Date_Fruit_Datasets.csv into a pandas dataframe. Print out the header. Use pandas.DataFrame.describe to summarize the data. Using markdown, explain the meaning of the columns (as well as you can with the information available) and make observations about the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Hunteracademic/Neural_network_group_7/master/Date_Fruit_Datasets.csv\"\n",
    "Fruit_data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422163</td>\n",
       "      <td>2378.908</td>\n",
       "      <td>837.8484</td>\n",
       "      <td>645.6693</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>733.1539</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>424428</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2370</td>\n",
       "      <td>2.9574</td>\n",
       "      <td>4.2287</td>\n",
       "      <td>-5.919126e+10</td>\n",
       "      <td>-50714214400</td>\n",
       "      <td>-39922372608</td>\n",
       "      <td>58.7255</td>\n",
       "      <td>54.9554</td>\n",
       "      <td>47.8400</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>338136</td>\n",
       "      <td>2085.144</td>\n",
       "      <td>723.8198</td>\n",
       "      <td>595.2073</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>656.1464</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>339014</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>1.2161</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6228</td>\n",
       "      <td>2.6350</td>\n",
       "      <td>3.1704</td>\n",
       "      <td>-3.423307e+10</td>\n",
       "      <td>-37462601728</td>\n",
       "      <td>-31477794816</td>\n",
       "      <td>50.0259</td>\n",
       "      <td>52.8168</td>\n",
       "      <td>47.8315</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>526843</td>\n",
       "      <td>2647.394</td>\n",
       "      <td>940.7379</td>\n",
       "      <td>715.3638</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>819.0222</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>528876</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>1.3150</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7516</td>\n",
       "      <td>3.8611</td>\n",
       "      <td>4.7192</td>\n",
       "      <td>-9.394835e+10</td>\n",
       "      <td>-74738221056</td>\n",
       "      <td>-60311207936</td>\n",
       "      <td>65.4772</td>\n",
       "      <td>59.2860</td>\n",
       "      <td>51.9378</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>416063</td>\n",
       "      <td>2351.210</td>\n",
       "      <td>827.9804</td>\n",
       "      <td>645.2988</td>\n",
       "      <td>0.6266</td>\n",
       "      <td>727.8378</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>418255</td>\n",
       "      <td>0.7759</td>\n",
       "      <td>1.2831</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0401</td>\n",
       "      <td>8.6136</td>\n",
       "      <td>8.2618</td>\n",
       "      <td>-3.207431e+10</td>\n",
       "      <td>-32060925952</td>\n",
       "      <td>-29575010304</td>\n",
       "      <td>43.3900</td>\n",
       "      <td>44.1259</td>\n",
       "      <td>41.1882</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>347562</td>\n",
       "      <td>2160.354</td>\n",
       "      <td>763.9877</td>\n",
       "      <td>582.8359</td>\n",
       "      <td>0.6465</td>\n",
       "      <td>665.2291</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>350797</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>1.3108</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7016</td>\n",
       "      <td>2.9761</td>\n",
       "      <td>4.4146</td>\n",
       "      <td>-3.998097e+10</td>\n",
       "      <td>-35980042240</td>\n",
       "      <td>-25593278464</td>\n",
       "      <td>52.7743</td>\n",
       "      <td>50.9080</td>\n",
       "      <td>42.6666</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
       "0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n",
       "1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n",
       "2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n",
       "3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n",
       "4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n",
       "\n",
       "   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n",
       "0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n",
       "1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n",
       "2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n",
       "3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n",
       "4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n",
       "\n",
       "   KurtosisRB     EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n",
       "0      4.2287 -5.919126e+10 -50714214400 -39922372608     58.7255     54.9554   \n",
       "1      3.1704 -3.423307e+10 -37462601728 -31477794816     50.0259     52.8168   \n",
       "2      4.7192 -9.394835e+10 -74738221056 -60311207936     65.4772     59.2860   \n",
       "3      8.2618 -3.207431e+10 -32060925952 -29575010304     43.3900     44.1259   \n",
       "4      4.4146 -3.998097e+10 -35980042240 -25593278464     52.7743     50.9080   \n",
       "\n",
       "   ALLdaub4RB  Class  \n",
       "0     47.8400  BERHI  \n",
       "1     47.8315  BERHI  \n",
       "2     51.9378  BERHI  \n",
       "3     41.1882  BERHI  \n",
       "4     42.6666  BERHI  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fruit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column description\n",
    "## Class\n",
    "This is the label or predicted output of the dataset. From some research at https://onlinelibrary.wiley.com/doi/10.1155/2021/4793293, We can find that each row in this dataset is a date and is broken into different types. \n",
    "| Date Type      | Class Label (Actual) | Color and Size                                                                                                       | Origins                                    |\n",
    "| -------------- | -------------------- | -------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| Barhee         | BERHI                | It is amber in color at harvest and then turns a golden-brown color. It is small to medium in size with a hard shell | Basra, Iraq                                |\n",
    "| Deglet Nour    | DEGLET               | It is a medium- to large-sized date fruits variety that matures from amber to dark brown after harvest               | Not specific                               |\n",
    "| Sukkary        | DOKOL                | It is golden in color and is a medium-sized date fruits variety                                                      | Al Qassim region, Saudi Arabia             |\n",
    "| Rotab Mozafati | IRAQI                | It has a full, dark brown appearance. It is a medium-sized and fleshy date variety                                   | Kerman, Iran                               |\n",
    "| Ruthana        | ROTANA               | It has brown and gold colors. It is a medium-sized date fruit variety                                                | Madinah, Saudi Arabia                      |\n",
    "| Safawi         | SAFAVI               | It has a dark black cherry color and the tips are brown. It is a medium-sized date fruit variety                     | Madina, Saudi Arabia                       |\n",
    "| Sagai          | SOGAY                | The tips are dry, golden in color, and the undersides are brown and soft. It is a medium-sized date variety          | Arabian Peninsula, especially Saudi Arabia |\n",
    "# Feature Description\n",
    "These columns represent features extracted from images of date fruits to classify them into seven genetic varieties: Barhee, Deglet Nour, Sukkary, Rotab Mozafati, Ruthana, Safawi, and Sagai. The dataset contains 898 images with 34 total features divided into morphological, shape, and color characteristics.\n",
    "## Structural Features\n",
    "| Feature      | Description                                                                                                                                |\n",
    "| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| Area         | The total number of pixels within the date fruit boundary, measuring the size of the fruit in the image.                                   |\n",
    "| Perimeter    | The total length of the date fruit's boundary in pixels, measuring the outline length.                                                     |\n",
    "| Major Axis   | The length of the longest axis through the date fruit when an ellipse is fitted to its shape, capturing the fruit's longest dimension.     |\n",
    "| Minor Axis   | The length of the shortest axis through the date fruit perpendicular to the major axis, capturing the fruit's width.                       |\n",
    "| Eccentricity | Measures how elongated the date fruit is (0 = perfectly circular, approaching 1 = highly elongated), calculated from the fitted ellipse.   |\n",
    "| EQDIASQ      | Equivalent Diameter Squared - the squared diameter of a circle with the same area as the date fruit.                                       |\n",
    "| Convex Area  | The area of the smallest convex polygon that completely encloses the date fruit, useful for detecting irregularities in fruit shape.       |\n",
    "| Extent       | The ratio of the date fruit's area to its bounding box area, indicating how much space the fruit occupies within its rectangular boundary. |\n",
    "## Shape Features\n",
    "| Feature       | Description                                                                                                                                             |\n",
    "| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Solidity      | The ratio of the fruit's actual area to its convex area, measuring how \"solid\" the date fruit is versus having surface indentations or irregular edges. |\n",
    "| Aspect Ratio  | The ratio of major axis to minor axis length, quantifying how elongated the date fruit is compared to its width.                                        |\n",
    "| Roundness     | A measure of how circular the date fruit appears, with values closer to 1 indicating more circular/spherical shapes.                                    |\n",
    "| Compactness   | A normalized circularity measure that describes how compact or spread out the fruit shape is.                                                           |\n",
    "| Shapefactor 1 | Mathematical formula combining area, perimeter, and axis measurements to capture geometric properties that distinguish date fruit varieties.            |\n",
    "| Shapefactor 2 | Mathematical formula combining area, perimeter, and axis measurements to capture geometric properties that distinguish date fruit varieties.            |\n",
    "| Shapefactor 3 | Mathematical formula combining area, perimeter, and axis measurements to capture geometric properties that distinguish date fruit varieties.            |\n",
    "| Shapefactor 4 | Mathematical formula combining area, perimeter, and axis measurements to capture geometric properties that distinguish date fruit varieties.            |\n",
    "## RR/RG/RB\n",
    "**Red, Green, and Blue** each feature title with RR/RG/RB is three different columns identifying different channels in an image \n",
    "| Feature           | Description                                                                                                                                                                                                        |\n",
    "| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| Mean RR/RG/RB     | The average pixel intensity in each color channel across the date fruit region, capturing the dominant color characteristics.                                                                                      |\n",
    "| STD DEV RR/RG/RB  | The standard deviation of pixel intensities in each channel, measuring color variation and uniformity across the fruit surface.                                                                                    |\n",
    "| Skew RR/RG/RB     | The asymmetry of the color intensity distribution in each channel, indicating whether the fruit has predominantly lighter or darker tones.                                                                         |\n",
    "| Kurtosis RR/RG/RB | Measures whether the color distribution is peaked or flat, capturing information about color uniformity versus variability.                                                                                        |\n",
    "| Entropy RR/RG/RB  | Shannon entropy quantifying the randomness or complexity of color patterns in each channel - higher values indicate more varied color textures on the fruit surface.                                               |\n",
    "| ALLdaub4 RR/RG/RB | Features extracted using Daubechies 4 (db4) wavelet transform applied to each color channel, capturing texture information at multiple scales that can distinguish subtle surface patterns between date varieties. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>SkewRB</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>298295.207127</td>\n",
       "      <td>2057.660953</td>\n",
       "      <td>750.811994</td>\n",
       "      <td>495.872785</td>\n",
       "      <td>0.737468</td>\n",
       "      <td>604.577938</td>\n",
       "      <td>0.981840</td>\n",
       "      <td>303845.592428</td>\n",
       "      <td>0.736267</td>\n",
       "      <td>2.131102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250518</td>\n",
       "      <td>4.247845</td>\n",
       "      <td>5.110894</td>\n",
       "      <td>3.780928</td>\n",
       "      <td>-3.185021e+10</td>\n",
       "      <td>-2.901860e+10</td>\n",
       "      <td>-2.771876e+10</td>\n",
       "      <td>50.082888</td>\n",
       "      <td>48.805681</td>\n",
       "      <td>48.098393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>107245.205337</td>\n",
       "      <td>410.012459</td>\n",
       "      <td>144.059326</td>\n",
       "      <td>114.268917</td>\n",
       "      <td>0.088727</td>\n",
       "      <td>119.593888</td>\n",
       "      <td>0.018157</td>\n",
       "      <td>108815.656947</td>\n",
       "      <td>0.053745</td>\n",
       "      <td>17.820778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632918</td>\n",
       "      <td>2.892357</td>\n",
       "      <td>3.745463</td>\n",
       "      <td>2.049831</td>\n",
       "      <td>2.037241e+10</td>\n",
       "      <td>1.712952e+10</td>\n",
       "      <td>1.484137e+10</td>\n",
       "      <td>16.063125</td>\n",
       "      <td>14.125911</td>\n",
       "      <td>10.813862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1987.000000</td>\n",
       "      <td>911.828000</td>\n",
       "      <td>336.722700</td>\n",
       "      <td>2.283200</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>50.298400</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>2257.000000</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>1.065300</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.029100</td>\n",
       "      <td>1.708200</td>\n",
       "      <td>1.607600</td>\n",
       "      <td>1.767200</td>\n",
       "      <td>-1.091220e+11</td>\n",
       "      <td>-9.261697e+10</td>\n",
       "      <td>-8.747177e+10</td>\n",
       "      <td>15.191100</td>\n",
       "      <td>20.524700</td>\n",
       "      <td>22.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>206948.000000</td>\n",
       "      <td>1726.091500</td>\n",
       "      <td>641.068650</td>\n",
       "      <td>404.684375</td>\n",
       "      <td>0.685625</td>\n",
       "      <td>513.317075</td>\n",
       "      <td>0.978825</td>\n",
       "      <td>210022.750000</td>\n",
       "      <td>0.705875</td>\n",
       "      <td>1.373725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196950</td>\n",
       "      <td>2.536625</td>\n",
       "      <td>2.508850</td>\n",
       "      <td>2.577275</td>\n",
       "      <td>-4.429444e+10</td>\n",
       "      <td>-3.894638e+10</td>\n",
       "      <td>-3.564534e+10</td>\n",
       "      <td>38.224425</td>\n",
       "      <td>38.654525</td>\n",
       "      <td>39.250725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>319833.000000</td>\n",
       "      <td>2196.345450</td>\n",
       "      <td>791.363400</td>\n",
       "      <td>495.054850</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>638.140950</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>327207.000000</td>\n",
       "      <td>0.746950</td>\n",
       "      <td>1.524150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>3.069800</td>\n",
       "      <td>3.127800</td>\n",
       "      <td>3.080700</td>\n",
       "      <td>-2.826156e+10</td>\n",
       "      <td>-2.620990e+10</td>\n",
       "      <td>-2.392928e+10</td>\n",
       "      <td>53.841300</td>\n",
       "      <td>50.337800</td>\n",
       "      <td>49.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>382573.000000</td>\n",
       "      <td>2389.716575</td>\n",
       "      <td>858.633750</td>\n",
       "      <td>589.031700</td>\n",
       "      <td>0.802150</td>\n",
       "      <td>697.930525</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>388804.000000</td>\n",
       "      <td>0.775850</td>\n",
       "      <td>1.674750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593950</td>\n",
       "      <td>4.449850</td>\n",
       "      <td>7.320400</td>\n",
       "      <td>4.283125</td>\n",
       "      <td>-1.460482e+10</td>\n",
       "      <td>-1.433105e+10</td>\n",
       "      <td>-1.660367e+10</td>\n",
       "      <td>63.063350</td>\n",
       "      <td>59.573600</td>\n",
       "      <td>56.666675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>546063.000000</td>\n",
       "      <td>2811.997100</td>\n",
       "      <td>1222.723000</td>\n",
       "      <td>766.453600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>833.827900</td>\n",
       "      <td>0.997400</td>\n",
       "      <td>552598.000000</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>535.525700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.092300</td>\n",
       "      <td>26.171100</td>\n",
       "      <td>26.736700</td>\n",
       "      <td>32.249500</td>\n",
       "      <td>-1.627316e+08</td>\n",
       "      <td>-5.627727e+08</td>\n",
       "      <td>-4.370435e+08</td>\n",
       "      <td>79.828900</td>\n",
       "      <td>83.064900</td>\n",
       "      <td>74.104600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AREA    PERIMETER   MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY  \\\n",
       "count     898.000000   898.000000   898.000000  898.000000    898.000000   \n",
       "mean   298295.207127  2057.660953   750.811994  495.872785      0.737468   \n",
       "std    107245.205337   410.012459   144.059326  114.268917      0.088727   \n",
       "min      1987.000000   911.828000   336.722700    2.283200      0.344800   \n",
       "25%    206948.000000  1726.091500   641.068650  404.684375      0.685625   \n",
       "50%    319833.000000  2196.345450   791.363400  495.054850      0.754700   \n",
       "75%    382573.000000  2389.716575   858.633750  589.031700      0.802150   \n",
       "max    546063.000000  2811.997100  1222.723000  766.453600      1.000000   \n",
       "\n",
       "          EQDIASQ    SOLIDITY    CONVEX_AREA      EXTENT  ASPECT_RATIO  ...  \\\n",
       "count  898.000000  898.000000     898.000000  898.000000    898.000000  ...   \n",
       "mean   604.577938    0.981840  303845.592428    0.736267      2.131102  ...   \n",
       "std    119.593888    0.018157  108815.656947    0.053745     17.820778  ...   \n",
       "min     50.298400    0.836600    2257.000000    0.512300      1.065300  ...   \n",
       "25%    513.317075    0.978825  210022.750000    0.705875      1.373725  ...   \n",
       "50%    638.140950    0.987300  327207.000000    0.746950      1.524150  ...   \n",
       "75%    697.930525    0.991800  388804.000000    0.775850      1.674750  ...   \n",
       "max    833.827900    0.997400  552598.000000    0.856200    535.525700  ...   \n",
       "\n",
       "           SkewRB  KurtosisRR  KurtosisRG  KurtosisRB     EntropyRR  \\\n",
       "count  898.000000  898.000000  898.000000  898.000000  8.980000e+02   \n",
       "mean     0.250518    4.247845    5.110894    3.780928 -3.185021e+10   \n",
       "std      0.632918    2.892357    3.745463    2.049831  2.037241e+10   \n",
       "min     -1.029100    1.708200    1.607600    1.767200 -1.091220e+11   \n",
       "25%     -0.196950    2.536625    2.508850    2.577275 -4.429444e+10   \n",
       "50%      0.135550    3.069800    3.127800    3.080700 -2.826156e+10   \n",
       "75%      0.593950    4.449850    7.320400    4.283125 -1.460482e+10   \n",
       "max      3.092300   26.171100   26.736700   32.249500 -1.627316e+08   \n",
       "\n",
       "          EntropyRG     EntropyRB  ALLdaub4RR  ALLdaub4RG  ALLdaub4RB  \n",
       "count  8.980000e+02  8.980000e+02  898.000000  898.000000  898.000000  \n",
       "mean  -2.901860e+10 -2.771876e+10   50.082888   48.805681   48.098393  \n",
       "std    1.712952e+10  1.484137e+10   16.063125   14.125911   10.813862  \n",
       "min   -9.261697e+10 -8.747177e+10   15.191100   20.524700   22.130000  \n",
       "25%   -3.894638e+10 -3.564534e+10   38.224425   38.654525   39.250725  \n",
       "50%   -2.620990e+10 -2.392928e+10   53.841300   50.337800   49.614100  \n",
       "75%   -1.433105e+10 -1.660367e+10   63.063350   59.573600   56.666675  \n",
       "max   -5.627727e+08 -4.370435e+08   79.828900   83.064900   74.104600  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fruit_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe\n",
    "- Perimeter ranges from 912 to 2,812 units with mean of 2,058.\n",
    "- No missing values (all counts = 898) this will be double checked when we look at null values. \n",
    "- EQDIASQ shows moderate spread (mean: 605, std: 120).\n",
    "- EntropyRR, EntropyRG, EntropyRB: All three have very high values.\n",
    "- ALLdaub4RR, ALLdaub4RG, ALLdaub4RB: Mean values around 48-50 with moderate standard deviations (10-16) suggest normalized color intensities\n",
    "- The max value for ASPECT_RATIO (and some other columns following it) being much higher than average and the quartile values suggest that there is a row with a very high value of 535.5 skewing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use pandas.DataFrame.info to check if the entries are the correct datatype, and if there are any missing values. Use pandas.DataFrame.duplicates to check for duplicate entries. Fix the dataset so that there are no missing values, duplicate rows, or incorrect data types. Use markdown to make observations and explain what you have done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 898 entries, 0 to 897\n",
      "Data columns (total 35 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   AREA           898 non-null    int64  \n",
      " 1   PERIMETER      898 non-null    float64\n",
      " 2   MAJOR_AXIS     898 non-null    float64\n",
      " 3   MINOR_AXIS     898 non-null    float64\n",
      " 4   ECCENTRICITY   898 non-null    float64\n",
      " 5   EQDIASQ        898 non-null    float64\n",
      " 6   SOLIDITY       898 non-null    float64\n",
      " 7   CONVEX_AREA    898 non-null    int64  \n",
      " 8   EXTENT         898 non-null    float64\n",
      " 9   ASPECT_RATIO   898 non-null    float64\n",
      " 10  ROUNDNESS      898 non-null    float64\n",
      " 11  COMPACTNESS    898 non-null    float64\n",
      " 12  SHAPEFACTOR_1  898 non-null    float64\n",
      " 13  SHAPEFACTOR_2  898 non-null    float64\n",
      " 14  SHAPEFACTOR_3  898 non-null    float64\n",
      " 15  SHAPEFACTOR_4  898 non-null    float64\n",
      " 16  MeanRR         898 non-null    float64\n",
      " 17  MeanRG         898 non-null    float64\n",
      " 18  MeanRB         898 non-null    float64\n",
      " 19  StdDevRR       898 non-null    float64\n",
      " 20  StdDevRG       898 non-null    float64\n",
      " 21  StdDevRB       898 non-null    float64\n",
      " 22  SkewRR         898 non-null    float64\n",
      " 23  SkewRG         898 non-null    float64\n",
      " 24  SkewRB         898 non-null    float64\n",
      " 25  KurtosisRR     898 non-null    float64\n",
      " 26  KurtosisRG     898 non-null    float64\n",
      " 27  KurtosisRB     898 non-null    float64\n",
      " 28  EntropyRR      898 non-null    float64\n",
      " 29  EntropyRG      898 non-null    int64  \n",
      " 30  EntropyRB      898 non-null    int64  \n",
      " 31  ALLdaub4RR     898 non-null    float64\n",
      " 32  ALLdaub4RG     898 non-null    float64\n",
      " 33  ALLdaub4RB     898 non-null    float64\n",
      " 34  Class          898 non-null    object \n",
      "dtypes: float64(30), int64(4), object(1)\n",
      "memory usage: 245.7+ KB\n"
     ]
    }
   ],
   "source": [
    "Fruit_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(Fruit_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AREA             0\n",
      "PERIMETER        0\n",
      "MAJOR_AXIS       0\n",
      "MINOR_AXIS       0\n",
      "ECCENTRICITY     0\n",
      "EQDIASQ          0\n",
      "SOLIDITY         0\n",
      "CONVEX_AREA      0\n",
      "EXTENT           0\n",
      "ASPECT_RATIO     0\n",
      "ROUNDNESS        0\n",
      "COMPACTNESS      0\n",
      "SHAPEFACTOR_1    0\n",
      "SHAPEFACTOR_2    0\n",
      "SHAPEFACTOR_3    0\n",
      "SHAPEFACTOR_4    0\n",
      "MeanRR           0\n",
      "MeanRG           0\n",
      "MeanRB           0\n",
      "StdDevRR         0\n",
      "StdDevRG         0\n",
      "StdDevRB         0\n",
      "SkewRR           0\n",
      "SkewRG           0\n",
      "SkewRB           0\n",
      "KurtosisRR       0\n",
      "KurtosisRG       0\n",
      "KurtosisRB       0\n",
      "EntropyRR        0\n",
      "EntropyRG        0\n",
      "EntropyRB        0\n",
      "ALLdaub4RR       0\n",
      "ALLdaub4RG       0\n",
      "ALLdaub4RB       0\n",
      "Class            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Fruit_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates, missing values or incorrect data types; so no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a bar plot using seaborn.barplot of the number of elements in each category. Use markdown to comment on how well balanced the dataset is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Class', ylabel='count'>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# countplot to count the class distribution, order it by size\n",
    "sns.countplot(Fruit_data,x='Class', palette='inferno', order=Fruit_data['Class'].value_counts().index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset shows us a pretty good distribution. DOKOL, SAFAVI, and ROTANA make up the majority of the dataset all with counts between 150 and 215. The remaining 4 classes DEGLET, SOGAY, IRAQI, and BERHI, make up the rest of the data all between 60 to 100. This overall gives us a balance distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Move the labels into a separate dataframe and use sklearn.preprocessing.LabelEncoder to convert the string labels into integers. Reshape the labels into a 2d array. Determine which number has been assigned to each type of date and record this information in markdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BERHI': 0,\n",
       " 'DEGLET': 1,\n",
       " 'DOKOL': 2,\n",
       " 'IRAQI': 3,\n",
       " 'ROTANA': 4,\n",
       " 'SAFAVI': 5,\n",
       " 'SOGAY': 6}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a separate dataframe for the features and the labels\n",
    "df_features = Fruit_data.drop(['Class'], axis = 1)\n",
    "labels = Fruit_data['Class']\n",
    "\n",
    "# Encoding the labels and converting the strings into integers\n",
    "encoder = LabelEncoder()\n",
    "labels_encoded = encoder.fit_transform(labels)\n",
    "\n",
    "# Reshaping encoded labels into a two-dimensional array\n",
    "labels_encoded = labels_encoded.reshape(-1, 1)\n",
    "labels_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "\n",
    "# Outputting the number assigned to each type\n",
    "labels_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use sklearn.preprocessing.MinMaxScaler to scale the features (but not the labels). Split the data into training, testing and validation sets with appropriate proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features \n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df_features)\n",
    "\n",
    "# Splitting the data into training, validation, and testing sets\n",
    "x_train, x_valtest, y_train, y_valtest = train_test_split(scaled_features, labels_encoded, train_size = 0.7, random_state = 10)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_valtest, y_valtest, train_size = 0.5, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((628, 34), (135, 34), (135, 34), (628, 1), (135, 1), (135, 1))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tf.keras.Sequential to create a fully connected artificial neural network with at least two hidden layers. Choose an activation function for each layer, and make sure the input and output dimensions are appropriate for the data. Print a summary of the model using tf.summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an artificial neural network with two hidden layers \n",
    "model = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)),  \n",
    "    Dense(units = 32, activation='relu'),                      \n",
    "    Dense(units = 7, activation='softmax')                      \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_112 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 7)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,551\n",
      "Trainable params: 4,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use softmax for final output layer with 7 neurons since we are doing multiclass classification. We have two hidden layers with 64 and 32 neurons respectively, using ReLU as the activation function. Since there are 34 features for every sample we have used 34 neurons for the first input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Compile the model with a choice of optimizer and loss function, and the set the metrics argument equal to ['accuracy'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing adam optimizer and sparse crossentropy (since this is multi-class) for loss\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train the model and record the training accuracy. Find the validation accuracy and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.9039 - accuracy: 0.2404 - val_loss: 1.7474 - val_accuracy: 0.5630\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.6563 - accuracy: 0.5876 - val_loss: 1.5061 - val_accuracy: 0.6593\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.4367 - accuracy: 0.6178 - val_loss: 1.2621 - val_accuracy: 0.6593\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.2477 - accuracy: 0.6178 - val_loss: 1.0780 - val_accuracy: 0.6593\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.0936 - accuracy: 0.6258 - val_loss: 0.9391 - val_accuracy: 0.6593\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.9778 - accuracy: 0.6385 - val_loss: 0.8355 - val_accuracy: 0.7333\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.8833 - accuracy: 0.6927 - val_loss: 0.7691 - val_accuracy: 0.7407\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.8053 - accuracy: 0.7261 - val_loss: 0.6957 - val_accuracy: 0.7407\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7470 - accuracy: 0.7564 - val_loss: 0.6534 - val_accuracy: 0.7481\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6981 - accuracy: 0.7675 - val_loss: 0.6067 - val_accuracy: 0.7852\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6533 - accuracy: 0.7930 - val_loss: 0.5821 - val_accuracy: 0.7852\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6194 - accuracy: 0.7882 - val_loss: 0.5525 - val_accuracy: 0.8074\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5905 - accuracy: 0.8010 - val_loss: 0.5292 - val_accuracy: 0.8222\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.8089 - val_loss: 0.5068 - val_accuracy: 0.8444\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5358 - accuracy: 0.8264 - val_loss: 0.5036 - val_accuracy: 0.8444\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.8264 - val_loss: 0.4823 - val_accuracy: 0.8444\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.8328 - val_loss: 0.4683 - val_accuracy: 0.8519\n",
      "Epoch 18/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.8185 - val_loss: 0.4578 - val_accuracy: 0.8519\n",
      "Epoch 19/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4751 - accuracy: 0.8519 - val_loss: 0.4461 - val_accuracy: 0.8667\n",
      "Epoch 20/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4586 - accuracy: 0.8487 - val_loss: 0.4395 - val_accuracy: 0.8667\n",
      "Epoch 21/25\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4583 - accuracy: 0.8344 - val_loss: 0.4402 - val_accuracy: 0.8593\n",
      "Epoch 22/25\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.4422 - accuracy: 0.8519 - val_loss: 0.4296 - val_accuracy: 0.8815\n",
      "Epoch 23/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8694 - val_loss: 0.4322 - val_accuracy: 0.8741\n",
      "Epoch 24/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4367 - accuracy: 0.8567 - val_loss: 0.4496 - val_accuracy: 0.8519\n",
      "Epoch 25/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4256 - accuracy: 0.8424 - val_loss: 0.4367 - val_accuracy: 0.8667\n"
     ]
    }
   ],
   "source": [
    "# Training the model on 25 epoch and a batch size of 32\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Monitor performance on validation set during training\n",
    "history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4256587028503418, 0.8630573153495789]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training accuracy\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4366900324821472, 0.8666666746139526]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 24,   0,   0,  15,   5,   0,   4],\n",
       "       [  0,  43,  13,   0,   0,   1,  15],\n",
       "       [  0,   1, 128,   0,   0,   0,   2],\n",
       "       [  2,   0,   0,  44,   1,   1,   0],\n",
       "       [  4,   0,   0,   0, 110,   0,   9],\n",
       "       [  0,   0,   0,   0,   0, 134,   2],\n",
       "       [  0,   4,   1,   0,   2,   4,  59]], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the confusion matrix\n",
    "y_train_probs = model.predict(x_train)\n",
    "y_train_pred = np.argmax(y_train_probs, axis=1)\n",
    "\n",
    "# True labels\n",
    "y_train_true = y_train.flatten()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_train_true, y_train_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 85.67% on the training set is pretty good, and 90.37% on the validation set means our model is performing well. As it is predicting better on the unseen data, that means it is generalizing pretty well. It is definitely not overfitting, so we could even try making it a little more complex to see if the accuracy improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to the above steps to try at least five different choices of hyperparameters (including dimensions, activation functions, number of layers, optimizer, loss function, etc.). Neatly present the description of each model tried along with the training and validation accuracies, and the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 1.7105 - accuracy: 0.4777 - val_loss: 1.3416 - val_accuracy: 0.6519\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.2053 - accuracy: 0.6162 - val_loss: 0.8758 - val_accuracy: 0.6593\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.8626 - accuracy: 0.6927 - val_loss: 0.6854 - val_accuracy: 0.7037\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.7691 - val_loss: 0.5463 - val_accuracy: 0.8519\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 0.8010 - val_loss: 0.4650 - val_accuracy: 0.8593\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4906 - accuracy: 0.8121 - val_loss: 0.4967 - val_accuracy: 0.8519\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4606 - accuracy: 0.8360 - val_loss: 0.4695 - val_accuracy: 0.8444\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.8073 - val_loss: 0.4458 - val_accuracy: 0.8741\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4354 - accuracy: 0.8201 - val_loss: 0.4140 - val_accuracy: 0.8815\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4176 - accuracy: 0.8344 - val_loss: 0.4221 - val_accuracy: 0.8963\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3925 - accuracy: 0.8551 - val_loss: 0.4087 - val_accuracy: 0.9037\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8567 - val_loss: 0.3950 - val_accuracy: 0.9111\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3533 - accuracy: 0.8662 - val_loss: 0.3899 - val_accuracy: 0.9111\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.8694 - val_loss: 0.3836 - val_accuracy: 0.9111\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3347 - accuracy: 0.8790 - val_loss: 0.3962 - val_accuracy: 0.9111\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8869 - val_loss: 0.3763 - val_accuracy: 0.9333\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3426 - accuracy: 0.8615 - val_loss: 0.3753 - val_accuracy: 0.9333\n",
      "Epoch 18/25\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3152 - accuracy: 0.8790 - val_loss: 0.3750 - val_accuracy: 0.9407\n",
      "Epoch 19/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3398 - accuracy: 0.8646 - val_loss: 0.3623 - val_accuracy: 0.9407\n",
      "Epoch 20/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.8933 - val_loss: 0.3776 - val_accuracy: 0.9333\n",
      "Epoch 21/25\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.8678 - val_loss: 0.4556 - val_accuracy: 0.8741\n",
      "Epoch 22/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2830 - accuracy: 0.8838 - val_loss: 0.3804 - val_accuracy: 0.9259\n",
      "Epoch 23/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2720 - accuracy: 0.9045 - val_loss: 0.3668 - val_accuracy: 0.9259\n",
      "Epoch 24/25\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.9029 - val_loss: 0.3612 - val_accuracy: 0.9333\n",
      "Epoch 25/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2851 - accuracy: 0.8869 - val_loss: 0.4306 - val_accuracy: 0.8963\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2937 - accuracy: 0.8965\n",
      "Training accuracy for second model:  [0.2936941683292389, 0.8964968323707581]\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8963\n",
      "Validation accuracy for second model:  [0.4306003749370575, 0.8962963223457336]\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "[[ 40   0   0   4   2   0   2]\n",
      " [  0  32  27   0   0   1  12]\n",
      " [  0   1 130   0   0   0   0]\n",
      " [  1   0   0  46   1   0   0]\n",
      " [  0   0   0   0 118   0   5]\n",
      " [  0   0   0   0   0 134   2]\n",
      " [  0   3   2   0   1   1  63]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves--adding additional layers and more units\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Building the second model\n",
    "model_2 = Sequential([\n",
    "    Dense(units = 256, activation='relu', input_shape=(34,)), \n",
    "    Dense(units = 128, activation='relu'),    \n",
    "    Dense(units = 64, activation='relu'),\n",
    "    Dense(units = 7, activation='softmax')                  \n",
    "])\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Monitor performance on validation set during training\n",
    "history = model_2.fit(x_train, y_train,epochs=25,batch_size=32,validation_data=(x_val, y_val),verbose=1)\n",
    "\n",
    "# Print out model evaluations\n",
    "print(\"Training accuracy for second model: \", model_2.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for second model: \", model_2.evaluate(x_val, y_val))\n",
    "\n",
    "# Generate predictions for model 2\n",
    "y_train_probs = model_2.predict(x_train)\n",
    "y_train_pred = np.argmax(y_train_probs, axis=1)\n",
    "y_train_true = y_train.flatten()\n",
    "cm = confusion_matrix(y_train_true, y_train_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already with additional layers and more neurons, although the complexity of our model increases, it performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7898 - accuracy: 0.4045 - val_loss: 1.5361 - val_accuracy: 0.6444\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.4759 - accuracy: 0.5987 - val_loss: 1.2617 - val_accuracy: 0.6519\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2451 - accuracy: 0.6131 - val_loss: 1.0444 - val_accuracy: 0.6593\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0741 - accuracy: 0.6210 - val_loss: 0.9034 - val_accuracy: 0.6667\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.9506 - accuracy: 0.6465 - val_loss: 0.8004 - val_accuracy: 0.6963\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.8625 - accuracy: 0.6592 - val_loss: 0.7250 - val_accuracy: 0.7333\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7881 - accuracy: 0.7293 - val_loss: 0.6722 - val_accuracy: 0.7556\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7305 - accuracy: 0.7484 - val_loss: 0.6269 - val_accuracy: 0.7778\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6861 - accuracy: 0.7771 - val_loss: 0.5897 - val_accuracy: 0.7926\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6433 - accuracy: 0.7834 - val_loss: 0.5545 - val_accuracy: 0.8222\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6100 - accuracy: 0.8089 - val_loss: 0.5270 - val_accuracy: 0.8222\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5787 - accuracy: 0.8185 - val_loss: 0.5071 - val_accuracy: 0.8370\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.8264 - val_loss: 0.4836 - val_accuracy: 0.8667\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5286 - accuracy: 0.8376 - val_loss: 0.4614 - val_accuracy: 0.8370\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5062 - accuracy: 0.8455 - val_loss: 0.4503 - val_accuracy: 0.8593\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4886 - accuracy: 0.8360 - val_loss: 0.4387 - val_accuracy: 0.8741\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4688 - accuracy: 0.8519 - val_loss: 0.4209 - val_accuracy: 0.8815\n",
      "Epoch 18/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.8583 - val_loss: 0.4026 - val_accuracy: 0.8741\n",
      "Epoch 19/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.8678 - val_loss: 0.3892 - val_accuracy: 0.9037\n",
      "Epoch 20/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8758 - val_loss: 0.3826 - val_accuracy: 0.9037\n",
      "Epoch 21/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4144 - accuracy: 0.8790 - val_loss: 0.3821 - val_accuracy: 0.8963\n",
      "Epoch 22/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8790 - val_loss: 0.3574 - val_accuracy: 0.8963\n",
      "Epoch 23/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8806 - val_loss: 0.3561 - val_accuracy: 0.9111\n",
      "Epoch 24/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8790 - val_loss: 0.3720 - val_accuracy: 0.9037\n",
      "Epoch 25/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8678 - val_loss: 0.3509 - val_accuracy: 0.9111\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8997\n",
      "Training accuracy for third model:  [0.3613485097885132, 0.8996815085411072]\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.9111\n",
      "Validation accuracy for third model:  [0.35094529390335083, 0.9111111164093018]\n",
      "20/20 [==============================] - 0s 774us/step\n",
      "[[ 34   0   0   7   3   0   4]\n",
      " [  0  45  15   0   0   1  11]\n",
      " [  0   3 128   0   0   0   0]\n",
      " [  1   0   0  45   2   0   0]\n",
      " [  0   0   0   0 117   0   6]\n",
      " [  0   0   0   0   0 135   1]\n",
      " [  0   3   2   0   1   3  61]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves: a different activation function\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Building the third model\n",
    "model_3 = Sequential([\n",
    "    Dense(units = 64, activation='tanh', input_shape=(34,)),\n",
    "    Dense(units = 32, activation='tanh'),    \n",
    "    Dense(units = 7, activation='softmax')\n",
    "])\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_3.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val, y_val), verbose=1) \n",
    "\n",
    "# Print evaluations\n",
    "print(\"Training accuracy for third model: \",model_3.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for third model: \",model_3.evaluate(x_val, y_val))\n",
    "\n",
    "# Generate predictions for model 3\n",
    "y_train_probs = model_3.predict(x_train)\n",
    "y_train_pred = np.argmax(y_train_probs, axis=1)\n",
    "y_train_true = y_train.flatten()\n",
    "cm = confusion_matrix(y_train_true, y_train_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tanh performs better than the baseline model but not as good as model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9402 - accuracy: 0.1990 - val_loss: 1.8525 - val_accuracy: 0.2370\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.8099 - accuracy: 0.3487 - val_loss: 1.7501 - val_accuracy: 0.5111\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7361 - accuracy: 0.5064 - val_loss: 1.6733 - val_accuracy: 0.5926\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.6796 - accuracy: 0.5637 - val_loss: 1.6151 - val_accuracy: 0.6519\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.6312 - accuracy: 0.5828 - val_loss: 1.5603 - val_accuracy: 0.6519\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.5829 - accuracy: 0.5971 - val_loss: 1.5122 - val_accuracy: 0.6593\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.5395 - accuracy: 0.6035 - val_loss: 1.4555 - val_accuracy: 0.6593\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4955 - accuracy: 0.6019 - val_loss: 1.4068 - val_accuracy: 0.6519\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4542 - accuracy: 0.6131 - val_loss: 1.3658 - val_accuracy: 0.6593\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4137 - accuracy: 0.6115 - val_loss: 1.3214 - val_accuracy: 0.6593\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3745 - accuracy: 0.6178 - val_loss: 1.2758 - val_accuracy: 0.6519\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3387 - accuracy: 0.6178 - val_loss: 1.2349 - val_accuracy: 0.6593\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3021 - accuracy: 0.6194 - val_loss: 1.2005 - val_accuracy: 0.6593\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2669 - accuracy: 0.6194 - val_loss: 1.1644 - val_accuracy: 0.6593\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2358 - accuracy: 0.6178 - val_loss: 1.1332 - val_accuracy: 0.6593\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.2050 - accuracy: 0.6210 - val_loss: 1.0996 - val_accuracy: 0.6593\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.1739 - accuracy: 0.6210 - val_loss: 1.0747 - val_accuracy: 0.6593\n",
      "Epoch 18/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.1479 - accuracy: 0.6210 - val_loss: 1.0422 - val_accuracy: 0.6593\n",
      "Epoch 19/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.1224 - accuracy: 0.6194 - val_loss: 1.0120 - val_accuracy: 0.6593\n",
      "Epoch 20/25\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.0974 - accuracy: 0.6210 - val_loss: 0.9886 - val_accuracy: 0.6593\n",
      "Epoch 21/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.0729 - accuracy: 0.6210 - val_loss: 0.9632 - val_accuracy: 0.6593\n",
      "Epoch 22/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.0496 - accuracy: 0.6226 - val_loss: 0.9434 - val_accuracy: 0.6593\n",
      "Epoch 23/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.0276 - accuracy: 0.6274 - val_loss: 0.9224 - val_accuracy: 0.6593\n",
      "Epoch 24/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.0047 - accuracy: 0.6290 - val_loss: 0.9021 - val_accuracy: 0.6667\n",
      "Epoch 25/25\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.9845 - accuracy: 0.6258 - val_loss: 0.8749 - val_accuracy: 0.6667\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.9705 - accuracy: 0.6354\n",
      "Training accuracy for fourth model:  [0.9705132842063904, 0.6353503465652466]\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.8749 - accuracy: 0.6667\n",
      "Validation accuracy for fourth model:  [0.874933123588562, 0.6666666865348816]\n",
      "20/20 [==============================] - 0s 947us/step\n",
      "[[  0   0   1   6  32   8   1]\n",
      " [  0   0  54   0  15   3   0]\n",
      " [  0   0 131   0   0   0   0]\n",
      " [  0   0   0   3  17  28   0]\n",
      " [  0   0   0   0 123   0   0]\n",
      " [  0   0   0   0   0 136   0]\n",
      " [  0   0  21   0  30  13   6]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves: trying out a different loss function\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Building model 4\n",
    "model_4 = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)), \n",
    "    Dense(units = 32, activation='relu'),    \n",
    "    Dense(units = 7, activation='softmax')                    \n",
    "])\n",
    "\n",
    "model_4.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_4.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_val, y_val), verbose=1)\n",
    "\n",
    "# Print evaluation\n",
    "print(\"Training accuracy for fourth model: \",model_4.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for fourth model: \",model_4.evaluate(x_val, y_val))\n",
    "\n",
    "# Generate predictions for model 4\n",
    "y_train_probs = model_4.predict(x_train)\n",
    "y_train_pred = np.argmax(y_train_probs, axis=1)\n",
    "y_train_true = y_train.flatten()\n",
    "cm = confusion_matrix(y_train_true, y_train_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD optimizer is not doing a great job of helping the model learn as our previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8439\n",
      "Training accuracy for fifth model and batch size 32:  [0.40507984161376953, 0.843949019908905]\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8815\n",
      "Validation accuracy for fifth model and batch size 32:  [0.4228811264038086, 0.8814814686775208]\n",
      "20/20 [==============================] - 0s 682us/step\n",
      "[[ 21   0   0  16   7   0   4]\n",
      " [  0  29  18   0   1   1  23]\n",
      " [  0   1 127   0   0   0   3]\n",
      " [  1   0   0  45   2   0   0]\n",
      " [  0   0   0   0 112   0  11]\n",
      " [  0   0   0   0   0 135   1]\n",
      " [  0   3   1   0   1   4  61]]\n",
      "20/20 [==============================] - 0s 871us/step - loss: 0.2949 - accuracy: 0.9029\n",
      "Training accuracy for fifth model and batch size 64:  [0.2948591709136963, 0.9028662443161011]\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.9111\n",
      "Validation accuracy for fifth model and batch size 64:  [0.3606279194355011, 0.9111111164093018]\n",
      "20/20 [==============================] - 0s 687us/step\n",
      "[[ 37   0   0   5   2   0   4]\n",
      " [  0  53   9   0   1   1   8]\n",
      " [  0   7 124   0   0   0   0]\n",
      " [  3   0   0  43   2   0   0]\n",
      " [  0   0   0   0 120   0   3]\n",
      " [  0   0   0   0   0 134   2]\n",
      " [  0   7   0   0   4   3  56]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves: larger batch size\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_5 = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)),\n",
    "    Dense(units = 32, activation='relu'),    \n",
    "    Dense(units = 7, activation='softmax')           \n",
    "])\n",
    "model_5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for batch_size in [32, 64]:\n",
    "    model_5.fit(x_train, y_train,epochs=25,batch_size=batch_size,validation_data=(x_val, y_val),verbose=0)\n",
    "    \n",
    "    print(f\"Training accuracy for fifth model and batch size {batch_size}: \", model_5.evaluate(x_train, y_train))\n",
    "    print(f\"Validation accuracy for fifth model and batch size {batch_size}: \", model_5.evaluate(x_val, y_val))\n",
    "    \n",
    "    y_train_probs = model_5.predict(x_train)\n",
    "    y_train_pred = np.argmax(y_train_probs, axis=1)\n",
    "    y_train_true = y_train.flatten()\n",
    "    cm = confusion_matrix(y_train_true, y_train_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A larger batch size performs slightly better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2311 - accuracy: 0.9156\n",
      "Training accuracy for sixth model and batch size 64:  [0.23106920719146729, 0.9156050682067871]\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.9407\n",
      "Validation accuracy for sixth model and batch size 64:  [0.3473986089229584, 0.9407407641410828]\n",
      "20/20 [==============================] - 0s 680us/step\n",
      "[[ 43   0   0   2   2   0   1]\n",
      " [  0  54   9   0   1   1   7]\n",
      " [  0   8 123   0   0   0   0]\n",
      " [  2   0   0  45   1   0   0]\n",
      " [  0   1   0   0 120   0   2]\n",
      " [  0   0   0   0   0 135   1]\n",
      " [  0   8   0   1   4   2  55]]\n"
     ]
    }
   ],
   "source": [
    "# Using final model; increase epochs\n",
    "tf.random.set_seed(42)\n",
    "# Building model 6\n",
    "model_6 = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)),\n",
    "    Dense(units = 32, activation='relu'),    \n",
    "    Dense(units = 7, activation='softmax')           \n",
    "])\n",
    "\n",
    "model_6.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print evaluation\n",
    "history = model_6.fit(x_train, y_train, epochs=94, batch_size=batch_size, validation_data=(x_val, y_val), verbose=0)\n",
    "    \n",
    "print(f\"Training accuracy for sixth model and batch size {batch_size}: \", model_6.evaluate(x_train, y_train))\n",
    "print(f\"Validation accuracy for sixth model and batch size {batch_size}: \", model_6.evaluate(x_val, y_val))\n",
    "\n",
    "# Generate prediction for model 6\n",
    "y_train_probs = model_6.predict(x_train)\n",
    "y_train_pred = np.argmax(y_train_probs, axis=1)\n",
    "y_train_true = y_train.flatten()\n",
    "cm = confusion_matrix(y_train_true, y_train_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of epochs to 94 balances using minimal computational power while not sacrificing accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since hyperparameter tuning increased the accuracy for various models, instead of choosing one we shall combine all the models that led to improvement and choose that one. This model will have more layers and a greater number of neurons, run more iterations, increase the batch size to get the best results. This increase in model capacity allows the network to better capture complex patterns in the data, leading to improved generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.8080 - accuracy: 0.4363 - val_loss: 1.5852 - val_accuracy: 0.6519\n",
      "Epoch 2/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4950 - accuracy: 0.6099 - val_loss: 1.2338 - val_accuracy: 0.6519\n",
      "Epoch 3/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.2161 - accuracy: 0.6131 - val_loss: 0.9554 - val_accuracy: 0.6593\n",
      "Epoch 4/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9744 - accuracy: 0.6338 - val_loss: 0.7839 - val_accuracy: 0.7037\n",
      "Epoch 5/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7951 - accuracy: 0.7213 - val_loss: 0.6353 - val_accuracy: 0.7407\n",
      "Epoch 6/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.7755 - val_loss: 0.5485 - val_accuracy: 0.8444\n",
      "Epoch 7/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5891 - accuracy: 0.8169 - val_loss: 0.4939 - val_accuracy: 0.8519\n",
      "Epoch 8/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5281 - accuracy: 0.8201 - val_loss: 0.4637 - val_accuracy: 0.8370\n",
      "Epoch 9/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4945 - accuracy: 0.8201 - val_loss: 0.4819 - val_accuracy: 0.8222\n",
      "Epoch 10/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4844 - accuracy: 0.8073 - val_loss: 0.4343 - val_accuracy: 0.8741\n",
      "Epoch 11/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.8487 - val_loss: 0.4149 - val_accuracy: 0.8667\n",
      "Epoch 12/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.8232 - val_loss: 0.4368 - val_accuracy: 0.8593\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4109 - accuracy: 0.8487 - val_loss: 0.4176 - val_accuracy: 0.8741\n",
      "Epoch 14/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3836 - accuracy: 0.8646 - val_loss: 0.3942 - val_accuracy: 0.8741\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3770 - accuracy: 0.8646 - val_loss: 0.4192 - val_accuracy: 0.8889\n",
      "Epoch 16/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8471 - val_loss: 0.4196 - val_accuracy: 0.8741\n",
      "Epoch 17/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3699 - accuracy: 0.8694 - val_loss: 0.3815 - val_accuracy: 0.8963\n",
      "Epoch 18/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3805 - accuracy: 0.8599 - val_loss: 0.3973 - val_accuracy: 0.8889\n",
      "Epoch 19/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3575 - accuracy: 0.8790 - val_loss: 0.3790 - val_accuracy: 0.9185\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3460 - accuracy: 0.8822 - val_loss: 0.3769 - val_accuracy: 0.8963\n",
      "Epoch 21/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3559 - accuracy: 0.8567 - val_loss: 0.3976 - val_accuracy: 0.9037\n",
      "Epoch 22/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3530 - accuracy: 0.8662 - val_loss: 0.3972 - val_accuracy: 0.8815\n",
      "Epoch 23/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8758 - val_loss: 0.4097 - val_accuracy: 0.8963\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3484 - accuracy: 0.8662 - val_loss: 0.4848 - val_accuracy: 0.8370\n",
      "Epoch 25/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3480 - accuracy: 0.8742 - val_loss: 0.4072 - val_accuracy: 0.8963\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8710 - val_loss: 0.3898 - val_accuracy: 0.8815\n",
      "Epoch 27/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.8790 - val_loss: 0.3706 - val_accuracy: 0.9185\n",
      "Epoch 28/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2909 - accuracy: 0.8997 - val_loss: 0.3527 - val_accuracy: 0.9333\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2890 - accuracy: 0.8981 - val_loss: 0.3572 - val_accuracy: 0.9259\n",
      "Epoch 30/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3025 - accuracy: 0.8774 - val_loss: 0.3736 - val_accuracy: 0.9111\n",
      "Epoch 31/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3068 - accuracy: 0.8854 - val_loss: 0.3535 - val_accuracy: 0.9259\n",
      "Epoch 32/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2840 - accuracy: 0.9013 - val_loss: 0.3542 - val_accuracy: 0.9333\n",
      "Epoch 33/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2850 - accuracy: 0.8949 - val_loss: 0.3848 - val_accuracy: 0.9185\n",
      "Epoch 34/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2713 - accuracy: 0.8949 - val_loss: 0.3599 - val_accuracy: 0.9185\n",
      "Epoch 35/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2776 - accuracy: 0.8901 - val_loss: 0.3474 - val_accuracy: 0.9259\n",
      "Epoch 36/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2653 - accuracy: 0.8949 - val_loss: 0.3429 - val_accuracy: 0.9333\n",
      "Epoch 37/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2760 - accuracy: 0.8917 - val_loss: 0.3453 - val_accuracy: 0.9259\n",
      "Epoch 38/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2696 - accuracy: 0.9029 - val_loss: 0.3371 - val_accuracy: 0.9481\n",
      "Epoch 39/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2574 - accuracy: 0.9092 - val_loss: 0.3431 - val_accuracy: 0.9407\n",
      "Epoch 40/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2586 - accuracy: 0.9045 - val_loss: 0.3448 - val_accuracy: 0.9407\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2552 - accuracy: 0.9156 - val_loss: 0.3577 - val_accuracy: 0.9111\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2545 - accuracy: 0.9156 - val_loss: 0.3549 - val_accuracy: 0.9407\n",
      "Epoch 43/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2406 - accuracy: 0.9045 - val_loss: 0.3733 - val_accuracy: 0.9259\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2302 - accuracy: 0.9124 - val_loss: 0.3317 - val_accuracy: 0.9407\n",
      "Epoch 45/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2538 - accuracy: 0.8997 - val_loss: 0.3451 - val_accuracy: 0.9259\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2385 - accuracy: 0.9108 - val_loss: 0.3355 - val_accuracy: 0.9481\n",
      "Epoch 47/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2288 - accuracy: 0.9188 - val_loss: 0.3253 - val_accuracy: 0.9481\n",
      "Epoch 48/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2254 - accuracy: 0.9108 - val_loss: 0.3485 - val_accuracy: 0.9333\n",
      "Epoch 49/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2296 - accuracy: 0.9092 - val_loss: 0.3528 - val_accuracy: 0.9259\n",
      "Epoch 50/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2355 - accuracy: 0.9108 - val_loss: 0.3223 - val_accuracy: 0.9481\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2255 - accuracy: 0.9076 - val_loss: 0.3548 - val_accuracy: 0.9333\n",
      "Epoch 52/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2182 - accuracy: 0.9172 - val_loss: 0.3257 - val_accuracy: 0.9481\n",
      "Epoch 53/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2106 - accuracy: 0.9188 - val_loss: 0.3386 - val_accuracy: 0.9481\n",
      "Epoch 54/120\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.2133 - accuracy: 0.9188 - val_loss: 0.3564 - val_accuracy: 0.9259\n",
      "Epoch 55/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2303 - accuracy: 0.9124 - val_loss: 0.3552 - val_accuracy: 0.9333\n",
      "Epoch 56/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2311 - accuracy: 0.9140 - val_loss: 0.3441 - val_accuracy: 0.9481\n",
      "Epoch 57/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2396 - accuracy: 0.9188 - val_loss: 0.3381 - val_accuracy: 0.9333\n",
      "Epoch 58/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2412 - accuracy: 0.9076 - val_loss: 0.3366 - val_accuracy: 0.9185\n",
      "Epoch 59/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2297 - accuracy: 0.8997 - val_loss: 0.3168 - val_accuracy: 0.9481\n",
      "Epoch 60/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.9172 - val_loss: 0.3434 - val_accuracy: 0.9481\n",
      "Epoch 61/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2070 - accuracy: 0.9220 - val_loss: 0.3395 - val_accuracy: 0.9407\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2141 - accuracy: 0.9061 - val_loss: 0.3216 - val_accuracy: 0.9333\n",
      "Epoch 63/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9172 - val_loss: 0.3361 - val_accuracy: 0.9259\n",
      "Epoch 64/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2005 - accuracy: 0.9172 - val_loss: 0.3278 - val_accuracy: 0.9407\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9299 - val_loss: 0.3239 - val_accuracy: 0.9481\n",
      "Epoch 66/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1934 - accuracy: 0.9188 - val_loss: 0.3068 - val_accuracy: 0.9481\n",
      "Epoch 67/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2159 - accuracy: 0.9076 - val_loss: 0.3121 - val_accuracy: 0.9407\n",
      "Epoch 68/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2263 - accuracy: 0.9140 - val_loss: 0.3446 - val_accuracy: 0.9407\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2011 - accuracy: 0.9188 - val_loss: 0.3144 - val_accuracy: 0.9481\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1875 - accuracy: 0.9236 - val_loss: 0.3182 - val_accuracy: 0.9481\n",
      "Epoch 71/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1870 - accuracy: 0.9268 - val_loss: 0.3223 - val_accuracy: 0.9556\n",
      "Epoch 72/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1872 - accuracy: 0.9315 - val_loss: 0.3117 - val_accuracy: 0.9407\n",
      "Epoch 73/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1882 - accuracy: 0.9283 - val_loss: 0.3481 - val_accuracy: 0.9407\n",
      "Epoch 74/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1820 - accuracy: 0.9363 - val_loss: 0.3163 - val_accuracy: 0.9481\n",
      "Epoch 75/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.9204 - val_loss: 0.3131 - val_accuracy: 0.9556\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1946 - accuracy: 0.9124 - val_loss: 0.3134 - val_accuracy: 0.9407\n",
      "Epoch 77/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1863 - accuracy: 0.9220 - val_loss: 0.3162 - val_accuracy: 0.9407\n",
      "Epoch 78/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1746 - accuracy: 0.9299 - val_loss: 0.3118 - val_accuracy: 0.9481\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.9315 - val_loss: 0.3488 - val_accuracy: 0.9259\n",
      "Epoch 80/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.9331 - val_loss: 0.3136 - val_accuracy: 0.9481\n",
      "Epoch 81/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1833 - accuracy: 0.9268 - val_loss: 0.3426 - val_accuracy: 0.9481\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1748 - accuracy: 0.9268 - val_loss: 0.3093 - val_accuracy: 0.9407\n",
      "Epoch 83/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.9283 - val_loss: 0.3231 - val_accuracy: 0.9481\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1772 - accuracy: 0.9331 - val_loss: 0.3141 - val_accuracy: 0.9481\n",
      "Epoch 85/120\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1899 - accuracy: 0.9204 - val_loss: 0.3364 - val_accuracy: 0.9333\n",
      "Epoch 86/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1949 - accuracy: 0.9204 - val_loss: 0.3320 - val_accuracy: 0.9333\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1943 - accuracy: 0.9220 - val_loss: 0.3284 - val_accuracy: 0.9481\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9315 - val_loss: 0.3318 - val_accuracy: 0.9259\n",
      "Epoch 89/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2034 - accuracy: 0.9156 - val_loss: 0.3571 - val_accuracy: 0.9333\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1934 - accuracy: 0.9061 - val_loss: 0.3268 - val_accuracy: 0.9407\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.9411 - val_loss: 0.3792 - val_accuracy: 0.9333\n",
      "Epoch 92/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1798 - accuracy: 0.9220 - val_loss: 0.3362 - val_accuracy: 0.9481\n",
      "Epoch 93/120\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9188 - val_loss: 0.3583 - val_accuracy: 0.9111\n",
      "Epoch 94/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1936 - accuracy: 0.9220 - val_loss: 0.3417 - val_accuracy: 0.9259\n",
      "Epoch 95/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2022 - accuracy: 0.9172 - val_loss: 0.3247 - val_accuracy: 0.9407\n",
      "Epoch 96/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1822 - accuracy: 0.9379 - val_loss: 0.3330 - val_accuracy: 0.9481\n",
      "Epoch 97/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.9315 - val_loss: 0.3445 - val_accuracy: 0.9481\n",
      "Epoch 98/120\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9252 - val_loss: 0.3103 - val_accuracy: 0.9407\n",
      "Epoch 99/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9252 - val_loss: 0.3074 - val_accuracy: 0.9407\n",
      "Epoch 100/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9268 - val_loss: 0.3575 - val_accuracy: 0.9407\n",
      "Epoch 101/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1760 - accuracy: 0.9299 - val_loss: 0.3353 - val_accuracy: 0.9556\n",
      "Epoch 102/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.9347 - val_loss: 0.3276 - val_accuracy: 0.9481\n",
      "Epoch 103/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1674 - accuracy: 0.9236 - val_loss: 0.3216 - val_accuracy: 0.9481\n",
      "Epoch 104/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1504 - accuracy: 0.9411 - val_loss: 0.3066 - val_accuracy: 0.9481\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1564 - accuracy: 0.9347 - val_loss: 0.3456 - val_accuracy: 0.9481\n",
      "Epoch 106/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9395 - val_loss: 0.3126 - val_accuracy: 0.9407\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9236 - val_loss: 0.3323 - val_accuracy: 0.9481\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9379 - val_loss: 0.3270 - val_accuracy: 0.9407\n",
      "Epoch 109/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1539 - accuracy: 0.9395 - val_loss: 0.3058 - val_accuracy: 0.9407\n",
      "Epoch 110/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1459 - accuracy: 0.9443 - val_loss: 0.3238 - val_accuracy: 0.9407\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9443 - val_loss: 0.3077 - val_accuracy: 0.9481\n",
      "Epoch 112/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1474 - accuracy: 0.9490 - val_loss: 0.3505 - val_accuracy: 0.9407\n",
      "Epoch 113/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1641 - accuracy: 0.9379 - val_loss: 0.3079 - val_accuracy: 0.9333\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1647 - accuracy: 0.9347 - val_loss: 0.3332 - val_accuracy: 0.9556\n",
      "Epoch 115/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1530 - accuracy: 0.9443 - val_loss: 0.3306 - val_accuracy: 0.9481\n",
      "Epoch 116/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9363 - val_loss: 0.3129 - val_accuracy: 0.9407\n",
      "Epoch 117/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1552 - accuracy: 0.9395 - val_loss: 0.3435 - val_accuracy: 0.9407\n",
      "Epoch 118/120\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 0.9443 - val_loss: 0.3209 - val_accuracy: 0.9407\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1441 - accuracy: 0.9475 - val_loss: 0.3221 - val_accuracy: 0.9481\n",
      "Epoch 120/120\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1511 - accuracy: 0.9363 - val_loss: 0.3031 - val_accuracy: 0.9407\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1475 - accuracy: 0.9490\n",
      "Training accuracy for final model:  [0.14750027656555176, 0.9490445852279663]\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.9407\n",
      "Validation accuracy for final model:  [0.30311939120292664, 0.9407407641410828]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_7 = Sequential([\n",
    "    Dense(units = 256, activation='relu', input_shape=(34,)), \n",
    "    Dense(units = 128, activation='relu'),    \n",
    "    Dense(units = 64, activation='relu'),\n",
    "    Dense(units = 7, activation='softmax')                  \n",
    "])\n",
    "model_7.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_7.fit(x_train, y_train, epochs=120, batch_size=64, validation_data=(x_val, y_val), verbose=1)\n",
    "\n",
    "print(\"Training accuracy for final model: \", model_7.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for final model: \", model_7.evaluate(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step\n",
      "Predicted: [2 1 3 4 5 2 2 2 4 4]\n",
      "Actual:    [2 1 3 4 5 1 2 2 4 4]\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.9185\n",
      "Testing accuracy for final model:  [0.23262785375118256, 0.9185185432434082]\n",
      "[[ 5  0  0  1  0  0  0]\n",
      " [ 0 11  3  0  0  0  1]\n",
      " [ 0  0 37  0  0  0  0]\n",
      " [ 2  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  4  0  0  0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "y_test_probs = model_7.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "y_test_true = y_test.flatten()\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "\n",
    "print(\"Predicted:\", y_test_pred[:10])\n",
    "print(\"Actual:   \", y_test_true[:10])\n",
    "print(\"Testing accuracy for final model: \", model_7.evaluate(x_test, y_test))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model performs well for the intended use case, achieving a testing accuracy of 91.85%, which indicates strong generalization to unseen data. While performance decreases slightly from training and validation accuracy, this behavior is expected and suggests that the model does not significantly overfit the training data.\n",
    "\n",
    "The confusion matrix shows that the model correctly classifies the majority of samples across all seven classes. Some misclassifications occur in a small subset of classes, likely due to overlapping feature patterns, which is common in multi-class classification problems.\n",
    "\n",
    "Overall, the model provides reliable predictions for the use case and is suitable for practical application. Further improvements could be achieved by refining feature selection or increasing data representation for classes that are more frequently confused."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
