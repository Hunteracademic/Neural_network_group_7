{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "Objective:\n",
    "To determine the variety of date fruit from data describing the colour, length, diameter, and shape.\n",
    "\n",
    "Data:\n",
    "Obtained from DATASETS (muratkoklu.com) and used in M. Koklu, R. Kursun, Y.S. Taspinar, and I. Cinar, \"Classification of Date Fruits into Genetic Varieties Using Image Analysis,\" Mathematical Problems in Engineering, Vol.2021, Article ID: 4793293 (2021).\n",
    "\n",
    "Problem Statement:\n",
    "In food production it is important to properly label ingredients for both health and business reasons. However, sometimes mistakes are made and there is room for improvement in food labeling practices. A number of different types of dates are grown around the world, and it takes expertise to correctly identify the variety. Your job as a machine learning developer is to create a model that can identify the type of date from external features such as colour, length, diameter and shape factors which have been determined by a computer vision model.\n",
    "\n",
    "Steps to be completed:\n",
    "Create a Jupyter notebook and complete the following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "19498c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5db4a8",
   "metadata": {},
   "source": [
    "\n",
    "Data\n",
    "Load Date_Fruit_Datasets.csv into a pandas dataframe. Print out the header. Use pandas.DataFrame.describe to summarize the data. Using markdown, explain the meaning of the columns (as well as you can with the information available) and make observations about the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "04c5e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Hunteracademic/Neural_network_group_7/master/Date_Fruit_Datasets.csv\"\n",
    "Fruit_data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "ad226bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>422163</td>\n",
       "      <td>2378.908</td>\n",
       "      <td>837.8484</td>\n",
       "      <td>645.6693</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>733.1539</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>424428</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2370</td>\n",
       "      <td>2.9574</td>\n",
       "      <td>4.2287</td>\n",
       "      <td>-5.919126e+10</td>\n",
       "      <td>-50714214400</td>\n",
       "      <td>-39922372608</td>\n",
       "      <td>58.7255</td>\n",
       "      <td>54.9554</td>\n",
       "      <td>47.8400</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>338136</td>\n",
       "      <td>2085.144</td>\n",
       "      <td>723.8198</td>\n",
       "      <td>595.2073</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>656.1464</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>339014</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>1.2161</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6228</td>\n",
       "      <td>2.6350</td>\n",
       "      <td>3.1704</td>\n",
       "      <td>-3.423307e+10</td>\n",
       "      <td>-37462601728</td>\n",
       "      <td>-31477794816</td>\n",
       "      <td>50.0259</td>\n",
       "      <td>52.8168</td>\n",
       "      <td>47.8315</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>526843</td>\n",
       "      <td>2647.394</td>\n",
       "      <td>940.7379</td>\n",
       "      <td>715.3638</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>819.0222</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>528876</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>1.3150</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7516</td>\n",
       "      <td>3.8611</td>\n",
       "      <td>4.7192</td>\n",
       "      <td>-9.394835e+10</td>\n",
       "      <td>-74738221056</td>\n",
       "      <td>-60311207936</td>\n",
       "      <td>65.4772</td>\n",
       "      <td>59.2860</td>\n",
       "      <td>51.9378</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>416063</td>\n",
       "      <td>2351.210</td>\n",
       "      <td>827.9804</td>\n",
       "      <td>645.2988</td>\n",
       "      <td>0.6266</td>\n",
       "      <td>727.8378</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>418255</td>\n",
       "      <td>0.7759</td>\n",
       "      <td>1.2831</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0401</td>\n",
       "      <td>8.6136</td>\n",
       "      <td>8.2618</td>\n",
       "      <td>-3.207431e+10</td>\n",
       "      <td>-32060925952</td>\n",
       "      <td>-29575010304</td>\n",
       "      <td>43.3900</td>\n",
       "      <td>44.1259</td>\n",
       "      <td>41.1882</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>347562</td>\n",
       "      <td>2160.354</td>\n",
       "      <td>763.9877</td>\n",
       "      <td>582.8359</td>\n",
       "      <td>0.6465</td>\n",
       "      <td>665.2291</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>350797</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>1.3108</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7016</td>\n",
       "      <td>2.9761</td>\n",
       "      <td>4.4146</td>\n",
       "      <td>-3.998097e+10</td>\n",
       "      <td>-35980042240</td>\n",
       "      <td>-25593278464</td>\n",
       "      <td>52.7743</td>\n",
       "      <td>50.9080</td>\n",
       "      <td>42.6666</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
       "0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n",
       "1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n",
       "2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n",
       "3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n",
       "4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n",
       "\n",
       "   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n",
       "0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n",
       "1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n",
       "2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n",
       "3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n",
       "4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n",
       "\n",
       "   KurtosisRB     EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n",
       "0      4.2287 -5.919126e+10 -50714214400 -39922372608     58.7255     54.9554   \n",
       "1      3.1704 -3.423307e+10 -37462601728 -31477794816     50.0259     52.8168   \n",
       "2      4.7192 -9.394835e+10 -74738221056 -60311207936     65.4772     59.2860   \n",
       "3      8.2618 -3.207431e+10 -32060925952 -29575010304     43.3900     44.1259   \n",
       "4      4.4146 -3.998097e+10 -35980042240 -25593278464     52.7743     50.9080   \n",
       "\n",
       "   ALLdaub4RB  Class  \n",
       "0     47.8400  BERHI  \n",
       "1     47.8315  BERHI  \n",
       "2     51.9378  BERHI  \n",
       "3     41.1882  BERHI  \n",
       "4     42.6666  BERHI  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fruit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "27f323c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>SkewRB</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>298295.207127</td>\n",
       "      <td>2057.660953</td>\n",
       "      <td>750.811994</td>\n",
       "      <td>495.872785</td>\n",
       "      <td>0.737468</td>\n",
       "      <td>604.577938</td>\n",
       "      <td>0.981840</td>\n",
       "      <td>303845.592428</td>\n",
       "      <td>0.736267</td>\n",
       "      <td>2.131102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250518</td>\n",
       "      <td>4.247845</td>\n",
       "      <td>5.110894</td>\n",
       "      <td>3.780928</td>\n",
       "      <td>-3.185021e+10</td>\n",
       "      <td>-2.901860e+10</td>\n",
       "      <td>-2.771876e+10</td>\n",
       "      <td>50.082888</td>\n",
       "      <td>48.805681</td>\n",
       "      <td>48.098393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>107245.205337</td>\n",
       "      <td>410.012459</td>\n",
       "      <td>144.059326</td>\n",
       "      <td>114.268917</td>\n",
       "      <td>0.088727</td>\n",
       "      <td>119.593888</td>\n",
       "      <td>0.018157</td>\n",
       "      <td>108815.656947</td>\n",
       "      <td>0.053745</td>\n",
       "      <td>17.820778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632918</td>\n",
       "      <td>2.892357</td>\n",
       "      <td>3.745463</td>\n",
       "      <td>2.049831</td>\n",
       "      <td>2.037241e+10</td>\n",
       "      <td>1.712952e+10</td>\n",
       "      <td>1.484137e+10</td>\n",
       "      <td>16.063125</td>\n",
       "      <td>14.125911</td>\n",
       "      <td>10.813862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1987.000000</td>\n",
       "      <td>911.828000</td>\n",
       "      <td>336.722700</td>\n",
       "      <td>2.283200</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>50.298400</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>2257.000000</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>1.065300</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.029100</td>\n",
       "      <td>1.708200</td>\n",
       "      <td>1.607600</td>\n",
       "      <td>1.767200</td>\n",
       "      <td>-1.091220e+11</td>\n",
       "      <td>-9.261697e+10</td>\n",
       "      <td>-8.747177e+10</td>\n",
       "      <td>15.191100</td>\n",
       "      <td>20.524700</td>\n",
       "      <td>22.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>206948.000000</td>\n",
       "      <td>1726.091500</td>\n",
       "      <td>641.068650</td>\n",
       "      <td>404.684375</td>\n",
       "      <td>0.685625</td>\n",
       "      <td>513.317075</td>\n",
       "      <td>0.978825</td>\n",
       "      <td>210022.750000</td>\n",
       "      <td>0.705875</td>\n",
       "      <td>1.373725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196950</td>\n",
       "      <td>2.536625</td>\n",
       "      <td>2.508850</td>\n",
       "      <td>2.577275</td>\n",
       "      <td>-4.429444e+10</td>\n",
       "      <td>-3.894638e+10</td>\n",
       "      <td>-3.564534e+10</td>\n",
       "      <td>38.224425</td>\n",
       "      <td>38.654525</td>\n",
       "      <td>39.250725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>319833.000000</td>\n",
       "      <td>2196.345450</td>\n",
       "      <td>791.363400</td>\n",
       "      <td>495.054850</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>638.140950</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>327207.000000</td>\n",
       "      <td>0.746950</td>\n",
       "      <td>1.524150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>3.069800</td>\n",
       "      <td>3.127800</td>\n",
       "      <td>3.080700</td>\n",
       "      <td>-2.826156e+10</td>\n",
       "      <td>-2.620990e+10</td>\n",
       "      <td>-2.392928e+10</td>\n",
       "      <td>53.841300</td>\n",
       "      <td>50.337800</td>\n",
       "      <td>49.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>382573.000000</td>\n",
       "      <td>2389.716575</td>\n",
       "      <td>858.633750</td>\n",
       "      <td>589.031700</td>\n",
       "      <td>0.802150</td>\n",
       "      <td>697.930525</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>388804.000000</td>\n",
       "      <td>0.775850</td>\n",
       "      <td>1.674750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593950</td>\n",
       "      <td>4.449850</td>\n",
       "      <td>7.320400</td>\n",
       "      <td>4.283125</td>\n",
       "      <td>-1.460482e+10</td>\n",
       "      <td>-1.433105e+10</td>\n",
       "      <td>-1.660367e+10</td>\n",
       "      <td>63.063350</td>\n",
       "      <td>59.573600</td>\n",
       "      <td>56.666675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>546063.000000</td>\n",
       "      <td>2811.997100</td>\n",
       "      <td>1222.723000</td>\n",
       "      <td>766.453600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>833.827900</td>\n",
       "      <td>0.997400</td>\n",
       "      <td>552598.000000</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>535.525700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.092300</td>\n",
       "      <td>26.171100</td>\n",
       "      <td>26.736700</td>\n",
       "      <td>32.249500</td>\n",
       "      <td>-1.627316e+08</td>\n",
       "      <td>-5.627727e+08</td>\n",
       "      <td>-4.370435e+08</td>\n",
       "      <td>79.828900</td>\n",
       "      <td>83.064900</td>\n",
       "      <td>74.104600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AREA    PERIMETER   MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY  \\\n",
       "count     898.000000   898.000000   898.000000  898.000000    898.000000   \n",
       "mean   298295.207127  2057.660953   750.811994  495.872785      0.737468   \n",
       "std    107245.205337   410.012459   144.059326  114.268917      0.088727   \n",
       "min      1987.000000   911.828000   336.722700    2.283200      0.344800   \n",
       "25%    206948.000000  1726.091500   641.068650  404.684375      0.685625   \n",
       "50%    319833.000000  2196.345450   791.363400  495.054850      0.754700   \n",
       "75%    382573.000000  2389.716575   858.633750  589.031700      0.802150   \n",
       "max    546063.000000  2811.997100  1222.723000  766.453600      1.000000   \n",
       "\n",
       "          EQDIASQ    SOLIDITY    CONVEX_AREA      EXTENT  ASPECT_RATIO  ...  \\\n",
       "count  898.000000  898.000000     898.000000  898.000000    898.000000  ...   \n",
       "mean   604.577938    0.981840  303845.592428    0.736267      2.131102  ...   \n",
       "std    119.593888    0.018157  108815.656947    0.053745     17.820778  ...   \n",
       "min     50.298400    0.836600    2257.000000    0.512300      1.065300  ...   \n",
       "25%    513.317075    0.978825  210022.750000    0.705875      1.373725  ...   \n",
       "50%    638.140950    0.987300  327207.000000    0.746950      1.524150  ...   \n",
       "75%    697.930525    0.991800  388804.000000    0.775850      1.674750  ...   \n",
       "max    833.827900    0.997400  552598.000000    0.856200    535.525700  ...   \n",
       "\n",
       "           SkewRB  KurtosisRR  KurtosisRG  KurtosisRB     EntropyRR  \\\n",
       "count  898.000000  898.000000  898.000000  898.000000  8.980000e+02   \n",
       "mean     0.250518    4.247845    5.110894    3.780928 -3.185021e+10   \n",
       "std      0.632918    2.892357    3.745463    2.049831  2.037241e+10   \n",
       "min     -1.029100    1.708200    1.607600    1.767200 -1.091220e+11   \n",
       "25%     -0.196950    2.536625    2.508850    2.577275 -4.429444e+10   \n",
       "50%      0.135550    3.069800    3.127800    3.080700 -2.826156e+10   \n",
       "75%      0.593950    4.449850    7.320400    4.283125 -1.460482e+10   \n",
       "max      3.092300   26.171100   26.736700   32.249500 -1.627316e+08   \n",
       "\n",
       "          EntropyRG     EntropyRB  ALLdaub4RR  ALLdaub4RG  ALLdaub4RB  \n",
       "count  8.980000e+02  8.980000e+02  898.000000  898.000000  898.000000  \n",
       "mean  -2.901860e+10 -2.771876e+10   50.082888   48.805681   48.098393  \n",
       "std    1.712952e+10  1.484137e+10   16.063125   14.125911   10.813862  \n",
       "min   -9.261697e+10 -8.747177e+10   15.191100   20.524700   22.130000  \n",
       "25%   -3.894638e+10 -3.564534e+10   38.224425   38.654525   39.250725  \n",
       "50%   -2.620990e+10 -2.392928e+10   53.841300   50.337800   49.614100  \n",
       "75%   -1.433105e+10 -1.660367e+10   63.063350   59.573600   56.666675  \n",
       "max   -5.627727e+08 -4.370435e+08   79.828900   83.064900   74.104600  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fruit_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "79d45bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 898 entries, 0 to 897\n",
      "Data columns (total 35 columns):\n",
      "AREA             898 non-null int64\n",
      "PERIMETER        898 non-null float64\n",
      "MAJOR_AXIS       898 non-null float64\n",
      "MINOR_AXIS       898 non-null float64\n",
      "ECCENTRICITY     898 non-null float64\n",
      "EQDIASQ          898 non-null float64\n",
      "SOLIDITY         898 non-null float64\n",
      "CONVEX_AREA      898 non-null int64\n",
      "EXTENT           898 non-null float64\n",
      "ASPECT_RATIO     898 non-null float64\n",
      "ROUNDNESS        898 non-null float64\n",
      "COMPACTNESS      898 non-null float64\n",
      "SHAPEFACTOR_1    898 non-null float64\n",
      "SHAPEFACTOR_2    898 non-null float64\n",
      "SHAPEFACTOR_3    898 non-null float64\n",
      "SHAPEFACTOR_4    898 non-null float64\n",
      "MeanRR           898 non-null float64\n",
      "MeanRG           898 non-null float64\n",
      "MeanRB           898 non-null float64\n",
      "StdDevRR         898 non-null float64\n",
      "StdDevRG         898 non-null float64\n",
      "StdDevRB         898 non-null float64\n",
      "SkewRR           898 non-null float64\n",
      "SkewRG           898 non-null float64\n",
      "SkewRB           898 non-null float64\n",
      "KurtosisRR       898 non-null float64\n",
      "KurtosisRG       898 non-null float64\n",
      "KurtosisRB       898 non-null float64\n",
      "EntropyRR        898 non-null float64\n",
      "EntropyRG        898 non-null int64\n",
      "EntropyRB        898 non-null int64\n",
      "ALLdaub4RR       898 non-null float64\n",
      "ALLdaub4RG       898 non-null float64\n",
      "ALLdaub4RB       898 non-null float64\n",
      "Class            898 non-null object\n",
      "dtypes: float64(30), int64(4), object(1)\n",
      "memory usage: 245.7+ KB\n"
     ]
    }
   ],
   "source": [
    "Fruit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e750efb",
   "metadata": {},
   "source": [
    "\n",
    "Use pandas.DataFrame.info to check if the entries are the correct datatype, and if there are any missing values. Use pandas.DataFrame.duplicates to check for duplicate entries. Fix the dataset so that there are no missing values, duplicate rows, or incorrect data types. Use markdown to make observations and explain what you have done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "d4d0d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(Fruit_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "b7d06291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AREA             0\n",
      "PERIMETER        0\n",
      "MAJOR_AXIS       0\n",
      "MINOR_AXIS       0\n",
      "ECCENTRICITY     0\n",
      "EQDIASQ          0\n",
      "SOLIDITY         0\n",
      "CONVEX_AREA      0\n",
      "EXTENT           0\n",
      "ASPECT_RATIO     0\n",
      "ROUNDNESS        0\n",
      "COMPACTNESS      0\n",
      "SHAPEFACTOR_1    0\n",
      "SHAPEFACTOR_2    0\n",
      "SHAPEFACTOR_3    0\n",
      "SHAPEFACTOR_4    0\n",
      "MeanRR           0\n",
      "MeanRG           0\n",
      "MeanRB           0\n",
      "StdDevRR         0\n",
      "StdDevRG         0\n",
      "StdDevRB         0\n",
      "SkewRR           0\n",
      "SkewRG           0\n",
      "SkewRB           0\n",
      "KurtosisRR       0\n",
      "KurtosisRG       0\n",
      "KurtosisRB       0\n",
      "EntropyRR        0\n",
      "EntropyRG        0\n",
      "EntropyRB        0\n",
      "ALLdaub4RR       0\n",
      "ALLdaub4RG       0\n",
      "ALLdaub4RB       0\n",
      "Class            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Fruit_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519eba0",
   "metadata": {},
   "source": [
    "\n",
    "Create a bar plot using seaborn.barplot of the number of elements in each category. Use markdown to comment on how well balanced the dataset is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2479116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c484af",
   "metadata": {},
   "source": [
    "\n",
    "Move the labels into a separate dataframe and use sklearn.preprocessing.LabelEncoder to convert the string labels into integers. Reshape the labels into a 2d array. Determine which number has been assigned to each type of date and record this information in markdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e70ebbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BERHI': 0,\n",
       " 'DEGLET': 1,\n",
       " 'DOKOL': 2,\n",
       " 'IRAQI': 3,\n",
       " 'ROTANA': 4,\n",
       " 'SAFAVI': 5,\n",
       " 'SOGAY': 6}"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a separate dataframe for the features and the labels\n",
    "df_features = Fruit_data.drop(['Class'], axis = 1)\n",
    "labels = Fruit_data['Class']\n",
    "\n",
    "# Encoding the labels and converting the strings into integers\n",
    "encoder = LabelEncoder()\n",
    "labels_encoded = encoder.fit_transform(labels)\n",
    "\n",
    "# Reshaping encoded labels into a two-dimensional array\n",
    "labels_encoded = labels_encoded.reshape(-1, 1)\n",
    "labels_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "\n",
    "# Outputting the number assigned to each type\n",
    "labels_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280761a",
   "metadata": {},
   "source": [
    "\n",
    "Use sklearn.preprocessing.MinMaxScaler to scale the features (but not the labels). Split the data into training, testing and validation sets with appropriate proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bec13296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features \n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df_features)\n",
    "\n",
    "# Splitting the data into training, validation, and testing sets\n",
    "x_train, x_valtest, y_train, y_valtest = train_test_split(scaled_features, labels_encoded, train_size = 0.7, random_state = 10)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_valtest, y_valtest, train_size = 0.5, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "8a00ab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((628, 34), (135, 34), (135, 34), (628, 1), (135, 1), (135, 1))"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e8e9e5",
   "metadata": {},
   "source": [
    "\n",
    "Modeling \n",
    "Use tf.keras.Sequential to create a fully connected artificial neural network with at least two hidden layers. Choose an activation function for each layer, and make sure the input and output dimensions are appropriate for the data. Print a summary of the model using tf.summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "10c900a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an artificial neural network with two hidden layers \n",
    "model = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)),  \n",
    "    Dense(units = 32, activation='relu'),                      \n",
    "    Dense(units = 7, activation='softmax')                      \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "9dc020be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_318 (Dense)            (None, 64)                2240      \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 4,551\n",
      "Trainable params: 4,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4817b7c",
   "metadata": {},
   "source": [
    "\n",
    "Compile the model with a choice of optimizer and loss function, and the set the metrics argument equal to ['accuracy'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "f5bac45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing adam optimizer and binary cross entropy for loss\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4d305",
   "metadata": {},
   "source": [
    "\n",
    "Train the model and record the training accuracy. Find the validation accuracy and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "995dd5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 628 samples, validate on 135 samples\n",
      "Epoch 1/25\n",
      "628/628 [==============================] - 0s 208us/sample - loss: 1.8305 - accuracy: 0.4618 - val_loss: 1.6897 - val_accuracy: 0.6444\n",
      "Epoch 2/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 1.6348 - accuracy: 0.6035 - val_loss: 1.4820 - val_accuracy: 0.6593\n",
      "Epoch 3/25\n",
      "628/628 [==============================] - 0s 25us/sample - loss: 1.4524 - accuracy: 0.6131 - val_loss: 1.2800 - val_accuracy: 0.6593\n",
      "Epoch 4/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 1.2751 - accuracy: 0.6146 - val_loss: 1.0958 - val_accuracy: 0.6593\n",
      "Epoch 5/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 1.1229 - accuracy: 0.6162 - val_loss: 0.9589 - val_accuracy: 0.6593\n",
      "Epoch 6/25\n",
      "628/628 [==============================] - 0s 26us/sample - loss: 1.0043 - accuracy: 0.6576 - val_loss: 0.8449 - val_accuracy: 0.7037\n",
      "Epoch 7/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.8975 - accuracy: 0.6704 - val_loss: 0.7533 - val_accuracy: 0.7333\n",
      "Epoch 8/25\n",
      "628/628 [==============================] - 0s 25us/sample - loss: 0.8164 - accuracy: 0.6975 - val_loss: 0.6840 - val_accuracy: 0.7333\n",
      "Epoch 9/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 0.7472 - accuracy: 0.7532 - val_loss: 0.6393 - val_accuracy: 0.7778\n",
      "Epoch 10/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 0.6916 - accuracy: 0.7818 - val_loss: 0.5872 - val_accuracy: 0.7778\n",
      "Epoch 11/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.6411 - accuracy: 0.8025 - val_loss: 0.5614 - val_accuracy: 0.7778\n",
      "Epoch 12/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 0.6081 - accuracy: 0.7962 - val_loss: 0.5191 - val_accuracy: 0.8370\n",
      "Epoch 13/25\n",
      "628/628 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.81 - 0s 23us/sample - loss: 0.5785 - accuracy: 0.8073 - val_loss: 0.5000 - val_accuracy: 0.8296\n",
      "Epoch 14/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.5410 - accuracy: 0.8376 - val_loss: 0.4790 - val_accuracy: 0.8370\n",
      "Epoch 15/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.5163 - accuracy: 0.8471 - val_loss: 0.4632 - val_accuracy: 0.8667\n",
      "Epoch 16/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4955 - accuracy: 0.8471 - val_loss: 0.4519 - val_accuracy: 0.8519\n",
      "Epoch 17/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4825 - accuracy: 0.8424 - val_loss: 0.4395 - val_accuracy: 0.8741\n",
      "Epoch 18/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 0.4605 - accuracy: 0.8519 - val_loss: 0.4388 - val_accuracy: 0.8667\n",
      "Epoch 19/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4549 - accuracy: 0.8519 - val_loss: 0.4313 - val_accuracy: 0.8519\n",
      "Epoch 20/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4421 - accuracy: 0.8455 - val_loss: 0.4193 - val_accuracy: 0.8667\n",
      "Epoch 21/25\n",
      "628/628 [==============================] - 0s 25us/sample - loss: 0.4249 - accuracy: 0.8535 - val_loss: 0.4093 - val_accuracy: 0.8889\n",
      "Epoch 22/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.4176 - accuracy: 0.8599 - val_loss: 0.4043 - val_accuracy: 0.8741\n",
      "Epoch 23/25\n",
      "628/628 [==============================] - 0s 25us/sample - loss: 0.4148 - accuracy: 0.8599 - val_loss: 0.4015 - val_accuracy: 0.8815\n",
      "Epoch 24/25\n",
      "628/628 [==============================] - 0s 25us/sample - loss: 0.3989 - accuracy: 0.8583 - val_loss: 0.3973 - val_accuracy: 0.8889\n",
      "Epoch 25/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 0.3928 - accuracy: 0.8583 - val_loss: 0.3935 - val_accuracy: 0.8815\n"
     ]
    }
   ],
   "source": [
    "# Training the model on 25 epoch and a batch size of 32\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "ba8e024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628/628 [==============================] - 0s 13us/sample - loss: 0.3805 - accuracy: 0.8774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3804717654255545, 0.87738854]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training accuracy\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "d294b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 0s 22us/sample - loss: 0.3935 - accuracy: 0.8815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39354448450936214, 0.88148147]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "56b9e261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  0,  2,  0,  0,  0],\n",
       "       [ 0,  7,  4,  0,  1,  0,  3],\n",
       "       [ 0,  1, 36,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0, 11,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0, 19,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 32,  0],\n",
       "       [ 0,  4,  0,  0,  1,  0,  8]], dtype=int64)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the confusion matrix\n",
    "y_test_probs = model.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "\n",
    "# True labels\n",
    "y_test_true = y_test.flatten()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6cf84",
   "metadata": {},
   "source": [
    "An accuracy of 87.5% on the training set is pretty good, and 88.9% on the validation set means our model is performing well. However, we'll try to tune the hyperparameters a bit more to see if the accuracy improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ea60b",
   "metadata": {},
   "source": [
    "Return to the above steps to try at least five different choices of hyperparameters (including dimensions, activation functions, number of layers, optimizer, loss function, etc.). Neatly present the description of each model tried along with the training and validation accuracies, and the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "8749c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 628 samples, validate on 135 samples\n",
      "Epoch 1/25\n",
      "628/628 [==============================] - 0s 496us/sample - loss: 1.7203 - accuracy: 0.4268 - val_loss: 1.3661 - val_accuracy: 0.6593\n",
      "Epoch 2/25\n",
      "628/628 [==============================] - 0s 32us/sample - loss: 1.2267 - accuracy: 0.6258 - val_loss: 0.9184 - val_accuracy: 0.6667\n",
      "Epoch 3/25\n",
      "628/628 [==============================] - 0s 32us/sample - loss: 0.8840 - accuracy: 0.6736 - val_loss: 0.6610 - val_accuracy: 0.7704\n",
      "Epoch 4/25\n",
      "628/628 [==============================] - 0s 32us/sample - loss: 0.6739 - accuracy: 0.7691 - val_loss: 0.5591 - val_accuracy: 0.8222\n",
      "Epoch 5/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.5642 - accuracy: 0.8121 - val_loss: 0.4984 - val_accuracy: 0.8519\n",
      "Epoch 6/25\n",
      "628/628 [==============================] - 0s 28us/sample - loss: 0.5080 - accuracy: 0.8217 - val_loss: 0.4499 - val_accuracy: 0.8370\n",
      "Epoch 7/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.4696 - accuracy: 0.8169 - val_loss: 0.4564 - val_accuracy: 0.8519\n",
      "Epoch 8/25\n",
      "628/628 [==============================] - 0s 31us/sample - loss: 0.4422 - accuracy: 0.8201 - val_loss: 0.4160 - val_accuracy: 0.8667\n",
      "Epoch 9/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.4137 - accuracy: 0.8471 - val_loss: 0.3907 - val_accuracy: 0.8741\n",
      "Epoch 10/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.3992 - accuracy: 0.8599 - val_loss: 0.3890 - val_accuracy: 0.8889\n",
      "Epoch 11/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.3935 - accuracy: 0.8439 - val_loss: 0.4637 - val_accuracy: 0.8593\n",
      "Epoch 12/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.3882 - accuracy: 0.8503 - val_loss: 0.3927 - val_accuracy: 0.9037\n",
      "Epoch 13/25\n",
      "628/628 [==============================] - 0s 28us/sample - loss: 0.3572 - accuracy: 0.8599 - val_loss: 0.4015 - val_accuracy: 0.8741\n",
      "Epoch 14/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.3804 - accuracy: 0.8487 - val_loss: 0.4922 - val_accuracy: 0.8593\n",
      "Epoch 15/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.3499 - accuracy: 0.8742 - val_loss: 0.3655 - val_accuracy: 0.9111\n",
      "Epoch 16/25\n",
      "628/628 [==============================] - 0s 31us/sample - loss: 0.3330 - accuracy: 0.8726 - val_loss: 0.4081 - val_accuracy: 0.8963\n",
      "Epoch 17/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.3325 - accuracy: 0.8535 - val_loss: 0.3657 - val_accuracy: 0.9111\n",
      "Epoch 18/25\n",
      "628/628 [==============================] - 0s 32us/sample - loss: 0.3033 - accuracy: 0.8949 - val_loss: 0.3962 - val_accuracy: 0.9037\n",
      "Epoch 19/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.2944 - accuracy: 0.8917 - val_loss: 0.3786 - val_accuracy: 0.9259\n",
      "Epoch 20/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.2858 - accuracy: 0.8933 - val_loss: 0.3841 - val_accuracy: 0.9185\n",
      "Epoch 21/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.2998 - accuracy: 0.8790 - val_loss: 0.3455 - val_accuracy: 0.9259\n",
      "Epoch 22/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.2825 - accuracy: 0.9092 - val_loss: 0.3258 - val_accuracy: 0.9259\n",
      "Epoch 23/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.3046 - accuracy: 0.8726 - val_loss: 0.4192 - val_accuracy: 0.8963\n",
      "Epoch 24/25\n",
      "628/628 [==============================] - 0s 33us/sample - loss: 0.2822 - accuracy: 0.8965 - val_loss: 0.3860 - val_accuracy: 0.9185\n",
      "Epoch 25/25\n",
      "628/628 [==============================] - 0s 35us/sample - loss: 0.2825 - accuracy: 0.8774 - val_loss: 0.3151 - val_accuracy: 0.9481\n",
      "628/628 [==============================] - 0s 13us/sample - loss: 0.2553 - accuracy: 0.9140\n",
      "Training accuracy for second model:  [0.2553414985252793, 0.91401273]\n",
      "135/135 [==============================] - 0s 22us/sample - loss: 0.3151 - accuracy: 0.9481\n",
      "Validation accuracy for second model:  [0.31509891505594606, 0.94814813]\n",
      "[[ 4  0  0  2  0  0  0]\n",
      " [ 0  7  3  0  0  0  5]\n",
      " [ 0  3 34  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  3  0  0  0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves--adding additional layers and more units\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Dense(units = 256, activation='relu', input_shape=(34,)), \n",
    "    Dense(units = 128, activation='relu'),    \n",
    "    Dense(units = 64, activation='relu'),\n",
    "    Dense(units = 7, activation='softmax')                  \n",
    "])\n",
    "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_2.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training accuracy for second model: \", model_2.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for second model: \", model_2.evaluate(x_val, y_val))\n",
    "\n",
    "y_test_probs = model_2.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "y_test_true = y_test.flatten()\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a1051",
   "metadata": {},
   "source": [
    "Already with additional layers and more neurons, although the complexity of our model increases, it performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "1ffb3cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 628 samples, validate on 135 samples\n",
      "Epoch 1/25\n",
      "628/628 [==============================] - 0s 201us/sample - loss: 1.7947 - accuracy: 0.3312 - val_loss: 1.5460 - val_accuracy: 0.6593\n",
      "Epoch 2/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 1.4775 - accuracy: 0.5987 - val_loss: 1.2603 - val_accuracy: 0.6519\n",
      "Epoch 3/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 1.2460 - accuracy: 0.6162 - val_loss: 1.0547 - val_accuracy: 0.6593\n",
      "Epoch 4/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 1.0767 - accuracy: 0.6258 - val_loss: 0.9084 - val_accuracy: 0.6815\n",
      "Epoch 5/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 0.9610 - accuracy: 0.6369 - val_loss: 0.8129 - val_accuracy: 0.6889\n",
      "Epoch 6/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 0.8738 - accuracy: 0.6927 - val_loss: 0.7376 - val_accuracy: 0.7259\n",
      "Epoch 7/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.7957 - accuracy: 0.6990 - val_loss: 0.6780 - val_accuracy: 0.7259\n",
      "Epoch 8/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 0.7387 - accuracy: 0.7484 - val_loss: 0.6315 - val_accuracy: 0.7630\n",
      "Epoch 9/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.6925 - accuracy: 0.7643 - val_loss: 0.6059 - val_accuracy: 0.8000\n",
      "Epoch 10/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.6531 - accuracy: 0.7834 - val_loss: 0.5636 - val_accuracy: 0.7926\n",
      "Epoch 11/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 0.6163 - accuracy: 0.8185 - val_loss: 0.5412 - val_accuracy: 0.8222\n",
      "Epoch 12/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.5889 - accuracy: 0.8137 - val_loss: 0.5049 - val_accuracy: 0.8519\n",
      "Epoch 13/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 0.5669 - accuracy: 0.8217 - val_loss: 0.4929 - val_accuracy: 0.8444\n",
      "Epoch 14/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.5364 - accuracy: 0.8280 - val_loss: 0.4734 - val_accuracy: 0.8519\n",
      "Epoch 15/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 0.5155 - accuracy: 0.8424 - val_loss: 0.4515 - val_accuracy: 0.8667\n",
      "Epoch 16/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4938 - accuracy: 0.8599 - val_loss: 0.4375 - val_accuracy: 0.8815\n",
      "Epoch 17/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.4784 - accuracy: 0.8408 - val_loss: 0.4232 - val_accuracy: 0.8889\n",
      "Epoch 18/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4575 - accuracy: 0.8583 - val_loss: 0.4144 - val_accuracy: 0.8963\n",
      "Epoch 19/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.4484 - accuracy: 0.8758 - val_loss: 0.4037 - val_accuracy: 0.8963\n",
      "Epoch 20/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4370 - accuracy: 0.8519 - val_loss: 0.3901 - val_accuracy: 0.8963\n",
      "Epoch 21/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 0.4186 - accuracy: 0.8790 - val_loss: 0.3726 - val_accuracy: 0.9111\n",
      "Epoch 22/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4085 - accuracy: 0.8774 - val_loss: 0.3656 - val_accuracy: 0.9037\n",
      "Epoch 23/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3961 - accuracy: 0.8806 - val_loss: 0.3582 - val_accuracy: 0.9037\n",
      "Epoch 24/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3835 - accuracy: 0.8869 - val_loss: 0.3546 - val_accuracy: 0.9037\n",
      "Epoch 25/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3768 - accuracy: 0.8790 - val_loss: 0.3453 - val_accuracy: 0.9185\n",
      "628/628 [==============================] - 0s 11us/sample - loss: 0.3676 - accuracy: 0.8933\n",
      "Training accuracy for third model:  [0.3676415193992056, 0.8933121]\n",
      "135/135 [==============================] - 0s 22us/sample - loss: 0.3453 - accuracy: 0.9185\n",
      "Validation accuracy for third model:  [0.3453342358271281, 0.91851854]\n",
      "[[ 4  0  0  2  0  0  0]\n",
      " [ 0  5  5  0  0  0  5]\n",
      " [ 0  1 36  0  0  0  0]\n",
      " [ 2  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  2  0  0  0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves: a different activation function\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_3 = Sequential([\n",
    "    Dense(units = 64, activation='tanh', input_shape=(34,)),\n",
    "    Dense(units = 32, activation='tanh'),    \n",
    "    Dense(units = 7, activation='softmax')\n",
    "])\n",
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_3.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training accuracy for third model: \",model_3.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for third model: \",model_3.evaluate(x_val, y_val))\n",
    "\n",
    "y_test_probs = model_3.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "y_test_true = y_test.flatten()\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b9722136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 628 samples, validate on 135 samples\n",
      "Epoch 1/25\n",
      "628/628 [==============================] - 0s 176us/sample - loss: 1.8151 - accuracy: 0.3248 - val_loss: 1.5841 - val_accuracy: 0.6444\n",
      "Epoch 2/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 1.4592 - accuracy: 0.5987 - val_loss: 1.2037 - val_accuracy: 0.6593\n",
      "Epoch 3/25\n",
      "628/628 [==============================] - 0s 23us/sample - loss: 1.1586 - accuracy: 0.6162 - val_loss: 0.9305 - val_accuracy: 0.6667\n",
      "Epoch 4/25\n",
      "628/628 [==============================] - 0s 25us/sample - loss: 0.9448 - accuracy: 0.6513 - val_loss: 0.7567 - val_accuracy: 0.6815\n",
      "Epoch 5/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.7991 - accuracy: 0.7038 - val_loss: 0.6821 - val_accuracy: 0.7704\n",
      "Epoch 6/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.7211 - accuracy: 0.7691 - val_loss: 0.5977 - val_accuracy: 0.7852\n",
      "Epoch 7/25\n",
      "628/628 [==============================] - 0s 24us/sample - loss: 0.6474 - accuracy: 0.7707 - val_loss: 0.5438 - val_accuracy: 0.8148\n",
      "Epoch 8/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.5989 - accuracy: 0.7850 - val_loss: 0.5008 - val_accuracy: 0.8444\n",
      "Epoch 9/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.5443 - accuracy: 0.8201 - val_loss: 0.4691 - val_accuracy: 0.8444\n",
      "Epoch 10/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.5392 - accuracy: 0.8185 - val_loss: 0.4541 - val_accuracy: 0.8519\n",
      "Epoch 11/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.5012 - accuracy: 0.8137 - val_loss: 0.5420 - val_accuracy: 0.7852\n",
      "Epoch 12/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4870 - accuracy: 0.7930 - val_loss: 0.4596 - val_accuracy: 0.8667\n",
      "Epoch 13/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.4756 - accuracy: 0.8217 - val_loss: 0.4629 - val_accuracy: 0.8370\n",
      "Epoch 14/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4672 - accuracy: 0.8312 - val_loss: 0.5212 - val_accuracy: 0.8222\n",
      "Epoch 15/25\n",
      "628/628 [==============================] - 0s 20us/sample - loss: 0.4425 - accuracy: 0.8392 - val_loss: 0.4053 - val_accuracy: 0.8815\n",
      "Epoch 16/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.4166 - accuracy: 0.8392 - val_loss: 0.4768 - val_accuracy: 0.8296\n",
      "Epoch 17/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.4295 - accuracy: 0.8248 - val_loss: 0.4116 - val_accuracy: 0.8741\n",
      "Epoch 18/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3944 - accuracy: 0.8599 - val_loss: 0.4265 - val_accuracy: 0.8741\n",
      "Epoch 19/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3807 - accuracy: 0.8631 - val_loss: 0.4225 - val_accuracy: 0.8741\n",
      "Epoch 20/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.3735 - accuracy: 0.8535 - val_loss: 0.4065 - val_accuracy: 0.8963\n",
      "Epoch 21/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.3797 - accuracy: 0.8471 - val_loss: 0.4006 - val_accuracy: 0.8889\n",
      "Epoch 22/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3458 - accuracy: 0.8806 - val_loss: 0.3814 - val_accuracy: 0.8963\n",
      "Epoch 23/25\n",
      "628/628 [==============================] - 0s 21us/sample - loss: 0.3888 - accuracy: 0.8408 - val_loss: 0.4677 - val_accuracy: 0.8444\n",
      "Epoch 24/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3684 - accuracy: 0.8583 - val_loss: 0.4316 - val_accuracy: 0.8593\n",
      "Epoch 25/25\n",
      "628/628 [==============================] - 0s 22us/sample - loss: 0.3501 - accuracy: 0.8615 - val_loss: 0.3627 - val_accuracy: 0.9185\n",
      "628/628 [==============================] - 0s 13us/sample - loss: 0.3164 - accuracy: 0.8933\n",
      "Training accuracy for fourth model:  [0.31638700149621174, 0.8933121]\n",
      "135/135 [==============================] - 0s 19us/sample - loss: 0.3627 - accuracy: 0.9185\n",
      "Validation accuracy for second model:  [0.36274767959559406, 0.91851854]\n",
      "[[ 4  0  0  1  1  0  0]\n",
      " [ 0  9  3  0  1  0  2]\n",
      " [ 0  3 34  0  0  0  0]\n",
      " [ 1  0  0 11  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  4  0  0  1  0  8]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves: trying out a different loss function\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_4 = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)), \n",
    "    Dense(units = 32, activation='relu'),    \n",
    "    Dense(units = 7, activation='softmax')                    \n",
    "])\n",
    "model_4.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_4.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training accuracy for fourth model: \",model_4.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for second model: \",model_4.evaluate(x_val, y_val))\n",
    "\n",
    "y_test_probs = model_4.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "y_test_true = y_test.flatten()\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "450b36f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628/628 [==============================] - 0s 12us/sample - loss: 0.3806 - accuracy: 0.8838\n",
      "Training accuracy for fifth model and batch size 32:  [0.3806121664442075, 0.88375795]\n",
      "135/135 [==============================] - 0s 22us/sample - loss: 0.3955 - accuracy: 0.8889\n",
      "Validation accuracy for fifth model and batch size 32:  [0.39549042516284516, 0.8888889]\n",
      "[[ 4  0  0  2  0  0  0]\n",
      " [ 0  7  4  0  1  0  3]\n",
      " [ 0  1 36  0  0  0  0]\n",
      " [ 2  0  0 10  0  0  0]\n",
      " [ 1  0  0  0 19  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  2  0  0  1  0 10]]\n",
      "628/628 [==============================] - 0s 11us/sample - loss: 0.3009 - accuracy: 0.9029\n",
      "Training accuracy for fifth model and batch size 64:  [0.30087077655610006, 0.90286624]\n",
      "135/135 [==============================] - 0s 22us/sample - loss: 0.3711 - accuracy: 0.9185\n",
      "Validation accuracy for fifth model and batch size 64:  [0.37112899555100337, 0.91851854]\n",
      "[[ 4  0  0  2  0  0  0]\n",
      " [ 0  7  4  0  1  0  3]\n",
      " [ 0  1 36  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  4  0  0  0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "# Creating another model to check if accuracy improves: larger batch size\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_5 = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)),\n",
    "    Dense(units = 32, activation='relu'),    \n",
    "    Dense(units = 7, activation='softmax')           \n",
    "])\n",
    "model_5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for batch_size in [32, 64]:\n",
    "    model_5.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "    print(f\"Training accuracy for fifth model and batch size {batch_size}: \", model_5.evaluate(x_train, y_train))\n",
    "    print(f\"Validation accuracy for fifth model and batch size {batch_size}: \", model_5.evaluate(x_val, y_val))\n",
    "    y_test_probs = model_5.predict(x_test)\n",
    "    y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "    y_test_true = y_test.flatten()\n",
    "    cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a645241",
   "metadata": {},
   "source": [
    "Extra model; increasing epochs on final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f59e289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628/628 [==============================] - 0s 10us/sample - loss: 0.2160 - accuracy: 0.9204\n",
      "Training accuracy for fifth model and batch size 32:  [0.21596202643433954, 0.92038214]\n",
      "135/135 [==============================] - 0s 19us/sample - loss: 0.3478 - accuracy: 0.9481\n",
      "Validation accuracy for fifth model and batch size 32:  [0.34781391046665333, 0.94814813]\n",
      "[[ 3  0  0  3  0  0  0]\n",
      " [ 0  8  3  0  0  0  4]\n",
      " [ 0  3 34  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  4  0  0  0  0  9]]\n",
      "628/628 [==============================] - 0s 11us/sample - loss: 0.1693 - accuracy: 0.9363\n",
      "Training accuracy for fifth model and batch size 64:  [0.1693055968565546, 0.93630576]\n",
      "135/135 [==============================] - 0s 22us/sample - loss: 0.3445 - accuracy: 0.9481\n",
      "Validation accuracy for fifth model and batch size 64:  [0.3445088236420243, 0.94814813]\n",
      "[[ 4  0  0  2  0  0  0]\n",
      " [ 0  8  3  0  0  0  4]\n",
      " [ 0  1 36  0  0  0  0]\n",
      " [ 1  0  0 11  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  2  0  0  0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "# Using final model; increase epochs\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_5 = Sequential([\n",
    "    Dense(units = 64, activation='relu', input_shape=(34,)),\n",
    "    Dense(units = 32, activation='relu'),    \n",
    "    Dense(units = 7, activation='softmax')           \n",
    "])\n",
    "model_5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for batch_size in [32, 64]:\n",
    "    model_5.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=94,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "    print(f\"Training accuracy for fifth model and batch size {batch_size}: \", model_5.evaluate(x_train, y_train))\n",
    "    print(f\"Validation accuracy for fifth model and batch size {batch_size}: \", model_5.evaluate(x_val, y_val))\n",
    "    y_test_probs = model_5.predict(x_test)\n",
    "    y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "    y_test_true = y_test.flatten()\n",
    "    cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281c225",
   "metadata": {},
   "source": [
    "When examining the validation accuracies across all experiments, it was observed that the best-performing model was the one that increased both the number of layers and the number of units per layer. This increase in model capacity allowed the network to better capture complex patterns in the data, leading to improved generalization on the validation set. Increasing the number of training epochs can also improve performance, as the model has more opportunities to minimize the loss function. However, this approach is more prone to overfitting, especially once the model begins to memorize the training data rather than learn meaningful patterns. Additionally, training for more epochs significantly increases computational time, making it less efficient for identifying the optimal model configuration compared to architectural improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "10a79302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 628 samples, validate on 135 samples\n",
      "Epoch 1/25\n",
      "628/628 [==============================] - 0s 232us/sample - loss: 1.7203 - accuracy: 0.4268 - val_loss: 1.3661 - val_accuracy: 0.6593\n",
      "Epoch 2/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 1.2267 - accuracy: 0.6258 - val_loss: 0.9184 - val_accuracy: 0.6667\n",
      "Epoch 3/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.8840 - accuracy: 0.6736 - val_loss: 0.6610 - val_accuracy: 0.7704\n",
      "Epoch 4/25\n",
      "628/628 [==============================] - 0s 31us/sample - loss: 0.6739 - accuracy: 0.7691 - val_loss: 0.5591 - val_accuracy: 0.8222\n",
      "Epoch 5/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.5642 - accuracy: 0.8121 - val_loss: 0.4984 - val_accuracy: 0.8519\n",
      "Epoch 6/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.5080 - accuracy: 0.8217 - val_loss: 0.4499 - val_accuracy: 0.8370\n",
      "Epoch 7/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.4696 - accuracy: 0.8169 - val_loss: 0.4564 - val_accuracy: 0.8519\n",
      "Epoch 8/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.4422 - accuracy: 0.8201 - val_loss: 0.4160 - val_accuracy: 0.8667\n",
      "Epoch 9/25\n",
      "628/628 [==============================] - 0s 28us/sample - loss: 0.4137 - accuracy: 0.8471 - val_loss: 0.3907 - val_accuracy: 0.8741\n",
      "Epoch 10/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.3992 - accuracy: 0.8599 - val_loss: 0.3890 - val_accuracy: 0.8889\n",
      "Epoch 11/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.3935 - accuracy: 0.8439 - val_loss: 0.4637 - val_accuracy: 0.8593\n",
      "Epoch 12/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.3882 - accuracy: 0.8503 - val_loss: 0.3927 - val_accuracy: 0.9037\n",
      "Epoch 13/25\n",
      "628/628 [==============================] - 0s 27us/sample - loss: 0.3572 - accuracy: 0.8599 - val_loss: 0.4015 - val_accuracy: 0.8741\n",
      "Epoch 14/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.3804 - accuracy: 0.8487 - val_loss: 0.4922 - val_accuracy: 0.8593\n",
      "Epoch 15/25\n",
      "628/628 [==============================] - 0s 27us/sample - loss: 0.3499 - accuracy: 0.8742 - val_loss: 0.3655 - val_accuracy: 0.9111\n",
      "Epoch 16/25\n",
      "628/628 [==============================] - 0s 27us/sample - loss: 0.3330 - accuracy: 0.8726 - val_loss: 0.4081 - val_accuracy: 0.8963\n",
      "Epoch 17/25\n",
      "628/628 [==============================] - 0s 27us/sample - loss: 0.3325 - accuracy: 0.8535 - val_loss: 0.3657 - val_accuracy: 0.9111\n",
      "Epoch 18/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.3033 - accuracy: 0.8949 - val_loss: 0.3962 - val_accuracy: 0.9037\n",
      "Epoch 19/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.2944 - accuracy: 0.8917 - val_loss: 0.3786 - val_accuracy: 0.9259\n",
      "Epoch 20/25\n",
      "628/628 [==============================] - 0s 27us/sample - loss: 0.2858 - accuracy: 0.8933 - val_loss: 0.3841 - val_accuracy: 0.9185\n",
      "Epoch 21/25\n",
      "628/628 [==============================] - 0s 30us/sample - loss: 0.2998 - accuracy: 0.8790 - val_loss: 0.3455 - val_accuracy: 0.9259\n",
      "Epoch 22/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.2825 - accuracy: 0.9092 - val_loss: 0.3258 - val_accuracy: 0.9259\n",
      "Epoch 23/25\n",
      "628/628 [==============================] - 0s 29us/sample - loss: 0.3046 - accuracy: 0.8726 - val_loss: 0.4192 - val_accuracy: 0.8963\n",
      "Epoch 24/25\n",
      "628/628 [==============================] - 0s 28us/sample - loss: 0.2822 - accuracy: 0.8965 - val_loss: 0.3860 - val_accuracy: 0.9185\n",
      "Epoch 25/25\n",
      "628/628 [==============================] - 0s 26us/sample - loss: 0.2825 - accuracy: 0.8774 - val_loss: 0.3151 - val_accuracy: 0.9481\n",
      "628/628 [==============================] - 0s 14us/sample - loss: 0.2553 - accuracy: 0.9140\n",
      "Training accuracy for second model:  [0.2553414985252793, 0.91401273]\n",
      "135/135 [==============================] - 0s 30us/sample - loss: 0.3151 - accuracy: 0.9481\n",
      "Validation accuracy for second model:  [0.31509891505594606, 0.94814813]\n",
      "[[ 4  0  0  2  0  0  0]\n",
      " [ 0  7  3  0  0  0  5]\n",
      " [ 0  3 34  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 32  0]\n",
      " [ 0  3  0  0  0  0 10]]\n",
      "Testing accuracy for second model: 0.88148147\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Dense(units = 256, activation='relu', input_shape=(34,)), \n",
    "    Dense(units = 128, activation='relu'),    \n",
    "    Dense(units = 64, activation='relu'),\n",
    "    Dense(units = 7, activation='softmax')                  \n",
    "])\n",
    "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_2.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training accuracy for second model: \", model_2.evaluate(x_train, y_train))\n",
    "print(\"Validation accuracy for second model: \", model_2.evaluate(x_val, y_val))\n",
    "\n",
    "y_test_probs = model_2.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
    "y_test_true = y_test.flatten()\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "test_loss, test_accuracy = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing accuracy for second model:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model performs well for the intended use case, achieving a testing accuracy of 88.1%, which indicates strong generalization to unseen data. While performance decreases slightly from training and validation accuracy, this behavior is expected and suggests that the model does not significantly overfit the training data.\n",
    "\n",
    "The confusion matrix shows that the model correctly classifies the majority of samples across all seven classes, with particularly strong performance in several classes that exhibit near-perfect prediction accuracy. Some misclassifications occur between a small subset of classes, likely due to overlapping feature patterns, which is common in multi-class classification problems.\n",
    "\n",
    "Overall, the model provides reliable predictions for the use case and is suitable for practical application. Further improvements could be achieved by refining feature selection or increasing data representation for classes that are more frequently confused."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
